import re
import json
import time
from typing import List
from openai import OpenAI
from typing import Dict, Generator
from .reference import ReferenceManager
from .word import TextCleaner

TASK_STATES = {}

def get_academic_thesis_prompt(target_words: int, ref_content_list: List[str], current_chapter_title: str, chapter_num: str) -> str:
    
    # ------------------------------------------------------------------
    # 1. ç« èŠ‚ä¸“å±é€»è¾‘
    # ------------------------------------------------------------------
    section_rule = ""
    is_abstract = "æ‘˜è¦" in current_chapter_title or "Abstract" in current_chapter_title
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºéœ€è¦å›¾è¡¨çš„ç« èŠ‚ (æ ¸å¿ƒä¿®å¤)
    # åªæœ‰åŒ…å«ä»¥ä¸‹å…³é”®è¯çš„ç« èŠ‚æ‰å¯ç”¨å›¾è¡¨
    needs_charts = False
    if chapter_num and any(k in current_chapter_title for k in ["å®éªŒ", "æµ‹è¯•", "åˆ†æ", "ç»“æœ", "æ•°æ®", "è®¾è®¡", "å®ç°", "éªŒè¯", "Evaluation", "Analysis", "Design"]):
        needs_charts = True
    
    # A. æ‘˜è¦
    if is_abstract:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™æ‘˜è¦ä¸å…³é”®è¯**
**é€»è¾‘ç»“æ„**:
1. **ç ”ç©¶èƒŒæ™¯**: ç®€è¿°èƒŒæ™¯ï¼ˆçº¦50å­—ï¼‰ã€‚
2. **æ–¹æ³•åˆ›æ–°**: åšäº†ä»€ä¹ˆï¼Œç”¨äº†ä»€ä¹ˆæ–¹æ³•ï¼ˆçº¦100å­—ï¼‰ã€‚
3. **å…³é”®å‘ç°**: å¾—åˆ°äº†ä»€ä¹ˆæ•°æ®æˆ–ç»“è®ºï¼ˆçº¦100å­—ï¼‰ã€‚
4. **ç†è®ºè´¡çŒ®**: ä»·å€¼æ˜¯ä»€ä¹ˆï¼ˆçº¦50å­—ï¼‰ã€‚

**è¾“å‡ºæ ¼å¼**:
### æ‘˜è¦
  [ä¸­æ–‡æ‘˜è¦å†…å®¹ï¼Œ350å­—å·¦å³]
**å…³é”®è¯**ï¼š[ä»é¢˜ç›®æå–3-5ä¸ªåè¯ï¼Œç”¨åˆ†å·éš”å¼€]

### Abstract
  [English Abstract, strictly corresponding]
**Keywords**: [English Keywords]
"""

    # B. èƒŒæ™¯ä¸æ„ä¹‰
    elif "èƒŒæ™¯" in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶èƒŒæ™¯**
**è¦æ±‚**:
1. **çœŸå®æ”¿ç­–**: å¿…é¡»ç»“åˆè¿‘å‡ å¹´ä¸­å›½çœŸå®å­˜åœ¨çš„å›½å®¶æ”¿ç­–ã€æœ€æ–°æ–‡ä»¶ã€é‡å¤§ç›¸å…³äº‹é¡¹ã€‚
2. **æ•°æ®æ”¯æ’‘**: éœ€è¦ä¸€ç‚¹çœŸå®æ•°æ®ä½œä¸ºèƒŒæ™¯æ”¯æ’‘ã€‚
3. **ç¯‡å¹…**: 350å­—å·¦å³ï¼Œä¸æ³›æ³›è€Œè°ˆã€‚
"""
    elif "æ„ä¹‰" in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶æ„ä¹‰**
**è¦æ±‚**:
1. **ç†è®ºæ„ä¹‰**: ä¸¥ç¦è¯´â€œå¡«è¡¥äº†ç©ºç™½â€ï¼Œå¿…é¡»è¯´â€œ**ä¸°å¯Œäº†...ç†è®ºæ¡†æ¶**â€æˆ–â€œ**ä¸º...æä¾›äº†å®è¯è¡¥å……**â€ã€‚
2. **å®é™…æ„ä¹‰**: è§£å†³å…·ä½“è¡Œä¸šæˆ–ç¤¾ä¼šç—›ç‚¹ã€‚
3. **ç¯‡å¹…**: 350å­—å·¦å³ã€‚
"""

# C. å›½å†…å¤–ç ”ç©¶ç°çŠ¶
    elif any(k in current_chapter_title for k in ["ç°çŠ¶", "ç»¼è¿°", "Review"]):
        if ref_content_list:
            first_ref = ref_content_list[0]
            # Pythoné€»è¾‘ä¿®å¤ï¼šå¿…é¡»ä¼ å…¥çœŸå®å†…å®¹ï¼Œè€ŒéIDå ä½ç¬¦
            if len(ref_content_list) > 1:
                other_refs_prompt = "\n".join([f"{{æ–‡çŒ®{i+2}}}: {ref}" for i, ref in enumerate(ref_content_list[1:])])
            else:
                other_refs_prompt = "æ— åç»­æ–‡çŒ®"
            
            section_rule = f"""
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶ç°çŠ¶ (æ–‡çŒ®ç»¼è¿°)**
**æ ¸å¿ƒç›®æ ‡**ï¼šå°†æä¾›çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨è½¬åŒ–ä¸ºé€»è¾‘é€šé¡ºçš„å­¦æœ¯è¯„è¿°ã€‚

### **å¼•ç”¨è§„èŒƒ (é›¶å®¹å¿è§„åˆ™)**
1.  **ç¦æ­¢å‡ºç°ID**: æ­£æ–‡ä¸­**ç»å¯¹ç¦æ­¢**å‡ºç° "å‚è€ƒæ–‡çŒ®ID"ã€"æ–‡çŒ®1"ã€"Reference ID" ç­‰å­—æ ·ã€‚
2.  **å¼•ç”¨æ ¼å¼**: å¿…é¡»ä»æ–‡çŒ®å†…å®¹ä¸­æå–**ä½œè€…**å’Œ**å¹´ä»½**ï¼Œæ ¼å¼ä¸º `ä½œè€…(å¹´ä»½)`ã€‚
    -   *ä¾‹*: "Zhang (2023) æå‡ºäº†..." æˆ– "OpenAI (2024) å‘å¸ƒäº†..."
    -   *å¦‚æœæ‰¾ä¸åˆ°ä½œè€…*: ä½¿ç”¨ `ã€Šæ ‡é¢˜ã€‹(å¹´ä»½)`ã€‚
3.  **ç¦æ­¢æ¨¡ç³Š**: ä¸¥ç¦ä½¿ç”¨ "æŸå­¦è€…"ã€"æœ‰ç ”ç©¶"ã€"è¯¥ä½œå“" ç­‰æŒ‡ä»£ä¸æ˜çš„è¯ï¼Œ**å¿…é¡»æŒ‡åé“å§“**ã€‚
4.  **é¡ºåºä¸é¢‘æ¬¡**: 
    -   å¿…é¡»**ä¸¥æ ¼æŒ‰ç…§åˆ—è¡¨é¡ºåº**é€ä¸€è®ºè¿°ã€‚
    -   åˆ—è¡¨ä¸­çš„**æ¯ä¸€æ¡**æ–‡çŒ®éƒ½å¿…é¡»è¢«å¼•ç”¨**ä¸€æ¬¡ä¸”ä»…ä¸€æ¬¡**ã€‚
    -   æ¯æ®µè®ºè¿°ç»“æŸå¥æœ«å°¾å¿…é¡»åŠ  `[REF]` æ ‡è®°ã€‚

### **å†™ä½œé€»è¾‘**
1.  **ç¬¬ä¸€æ®µ (å¯¼è¯­)**: ç®€è¦æ¦‚æ‹¬è¯¥é¢†åŸŸçš„æ€»ä½“å‘å±•è¶‹åŠ¿ï¼ˆçº¦80å­—ï¼‰ã€‚
2.  **ç¬¬äºŒæ®µ (æ ¸å¿ƒç»¼è¿°)**: 
    -   **é¦–æ¡è¯¦è¿°**: é’ˆå¯¹ **{{æ–‡çŒ®1}}** ({first_ref}) è¿›è¡Œè¯¦ç»†è¯„è¿°ï¼ˆçº¦150å­—ï¼‰ã€‚å†™æ˜ï¼šä½œè€…+å¹´ä»½+æ ¸å¿ƒè´¡çŒ®+å±€é™æ€§ã€‚æ–‡æœ«åŠ  `[REF]`ã€‚
    -   **åç»­ä¸²è”**: ä¾æ¬¡å¯¹ **{{æ–‡çŒ®2}}** åŠåç»­æ–‡çŒ®è¿›è¡Œè¯„è¿°ã€‚
        -   *å¿…é¡»ä»æä¾›çš„æ–‡æœ¬ä¸­æå–çœŸå®ä½œè€…å’Œè§‚ç‚¹ï¼Œä¸¥ç¦ç¼–é€ ã€‚*
        -   ä½¿ç”¨è¿æ¥è¯ï¼ˆå¦‚"ä¸ä¹‹ç±»ä¼¼"ã€"ç„¶è€Œ"ã€"åœ¨æ­¤åŸºç¡€ä¸Š"ï¼‰å°†ä¸åŒæ–‡çŒ®é€»è¾‘ä¸²è”ã€‚
        -   æ ¼å¼ï¼š`ä½œè€…(å¹´ä»½) + è§‚ç‚¹/æ–¹æ³• + [REF]`ã€‚
3.  **ç¬¬ä¸‰æ®µ (è¯„è¿°)**: æ€»ç»“ä¸Šè¿°æ–‡çŒ®çš„å…±åŒä¸è¶³ï¼Œå¼•å‡ºæœ¬ç ”ç©¶çš„åˆ‡å…¥ç‚¹ã€‚

**å¾…ç»¼è¿°çš„æ–‡çŒ®åˆ—è¡¨ (è¯·ä»ä¸­æå–ä¿¡æ¯)**:
- {{æ–‡çŒ®1}}: {first_ref}
{other_refs_prompt}
"""
        else:
            section_rule = "**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶ç°çŠ¶**\nè¯·åŸºäºé€šç”¨å­¦æœ¯çŸ¥è¯†æ’°å†™ï¼Œä¿æŒæ€»åˆ†æ€»ç»“æ„ï¼Œå¼•ç”¨çœŸå®å­˜åœ¨çš„ç»å…¸æ–‡çŒ®ã€‚"

    # D. æ–‡çŒ®è¿°è¯„
    elif "è¿°è¯„" in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™æ–‡çŒ®è¿°è¯„**
**è¦æ±‚**: 
1. **ä¸å¼•ç”¨**: æ­¤éƒ¨åˆ†ä¸éœ€è¦å¼•ç”¨å…·ä½“æ–‡çŒ®ã€‚
2. **å†…å®¹**: æ€»ç»“å‰æ–‡æ–‡çŒ®çš„ä¸è¶³ï¼ŒæŒ‡å‡ºæœ¬ç ”ç©¶çš„åˆ‡å…¥ç‚¹ï¼ˆå€Ÿé‰´ä»€ä¹ˆï¼Œä¸°å¯Œä»€ä¹ˆï¼‰ã€‚
3. **ç¯‡å¹…**: ä¸€ä¸ªæ®µè½ï¼Œ300å­—å·¦å³ã€‚
"""

    # E. ç ”ç©¶å†…å®¹
    elif "ç ”ç©¶å†…å®¹" in current_chapter_title and "æ–¹æ³•" not in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶å†…å®¹**
**æ ¼å¼**: åˆ†æ®µå¼å›ç­”ã€‚
1. **å¯¼è¯­**: â€œæœ¬ç ”ç©¶ä¸»è¦ç ”ç©¶...ï¼Œå…·ä½“å†…å®¹å¦‚ä¸‹ï¼šâ€
2. **åˆ†ç« èŠ‚**: 
   - â€œç¬¬ä¸€éƒ¨åˆ†ï¼Œç»ªè®ºã€‚ä¸»è¦é˜è¿°...â€
   - â€œç¬¬äºŒéƒ¨åˆ†ï¼Œ...ã€‚åˆ†æäº†...â€
   - ...
**è¦æ±‚**: æ ¸å¿ƒç« èŠ‚è§£é‡Šçº¦200å­—ï¼Œè¯¦ç•¥å¾—å½“ã€‚
"""

    # F. ç ”ç©¶æ–¹æ³•
    elif "ç ”ç©¶æ–¹æ³•" in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶æ–¹æ³•**
**æ ¼å¼**: åˆ†ç‚¹å›ç­”ï¼Œå¿…é¡»æ ‡åºå· (1. 2. 3.)ã€‚
**å¿…é€‰æ–¹æ³• (æŒ‰éœ€é€‰æ‹©)**:
1. **æ–‡çŒ®ç ”ç©¶æ³•**: (å¦‚æœ‰å‚è€ƒæ–‡çŒ®åˆ™å¿…é€‰)
2. **æ•°æ®åˆ†ææ³•**: (å¦‚æœ‰æ•°æ®åˆ†æåˆ™å¿…é€‰)
3. **å®è¯ç ”ç©¶æ³•/æ¡ˆä¾‹åˆ†ææ³•**: (æ ¹æ®é¢˜ç›®åˆ¤æ–­)
**è¦æ±‚**: ç»“åˆè®ºæ–‡ä¸»é¢˜è§£é‡Šä¸ºä»€ä¹ˆç”¨è¿™ä¸ªæ–¹æ³•ã€‚
"""

    # G. é€šç”¨æ­£æ–‡ (æ–°å¢æ•°æ®åˆ†æè¦æ±‚)
    else:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™æ­£æ–‡åˆ†æ**
1. **é€»è¾‘ä¸»å¯¼**: æ ¸å¿ƒæ˜¯åˆ†ææ€è·¯ã€‚
2. **æ·±åº¦è®ºè¿°**: æ¯ä¸€æ®µéƒ½è¦æœ‰è§‚ç‚¹ã€æœ‰è®ºæ®ï¼ˆæ•°æ®æˆ–ç†è®ºï¼‰ã€æœ‰ç»“è®ºã€‚
"""

    # ------------------------------------------------------------------
    # 2. å¼•ç”¨æŒ‡ä»¤
    # ------------------------------------------------------------------
    ref_instruction = ""
    if ref_content_list and any(k in current_chapter_title for k in ["ç°çŠ¶", "ç»¼è¿°", "Review"]):
        ref_instruction = f"""
### **ç­–ç•¥D: å¼•ç”¨æ‰§è¡Œ (Token Strategy)**
æœ¬ç« èŠ‚å¿…é¡»å¼•ç”¨åˆ†é…çš„ {len(ref_content_list)} æ¡æ–‡çŒ®ã€‚
1.  **ä¸è¦ç”Ÿæˆåºå·**: ä¸è¦å†™ [1] [2]ã€‚
2.  **æ’å…¥æ ‡è®°**: åœ¨æåˆ°æ–‡çŒ®è§‚ç‚¹æ—¶ï¼Œæ’å…¥ **`[REF]`**ã€‚
3.  **æ•°é‡**: å¿…é¡»æ’å…¥ {len(ref_content_list)} ä¸ª `[REF]` æ ‡è®°ã€‚
4.  **å…³è”**: å³ä½¿æ–‡çŒ®ä¸ç›¸å…³ï¼Œä¹Ÿè¦ç”¨â€œæ­¤å¤–ï¼Œä¹Ÿæœ‰ç ”ç©¶æŒ‡å‡º...â€å¼ºè¡Œå…³è”ï¼Œ**è‡ªåœ†å…¶è¯´**ã€‚
"""
    else:
        ref_instruction = "### **ç­–ç•¥D: å¼•ç”¨ç­–ç•¥**\næœ¬ç« èŠ‚æ— éœ€å¼ºåˆ¶å¼•ç”¨åˆ—è¡¨ä¸­çš„æ–‡çŒ®ï¼Œå¦‚éœ€å¼•ç”¨æ•°æ®è¯·ä½¿ç”¨çœŸå®çŸ¥è¯†ã€‚"

    word_count_strategy = f"ç›®æ ‡: **{target_words} å­—**ã€‚" if not is_abstract else "å­—æ•°ç›®æ ‡ä»…é€‚ç”¨äºä¸­æ–‡éƒ¨åˆ†ã€‚"

    # ----------------- ç­–ç•¥F: æ›´æ–°ä¸º Python ç»˜å›¾ -----------------
    visuals_instruction = ""
    if needs_charts:
        visuals_instruction = f"""
### **ç­–ç•¥F: å›¾è¡¨ä¸æ•°æ®å¯è§†åŒ– (Python & Tables)**
**æœ¬ç« èŠ‚å¿…é¡»åŒ…å«å›¾è¡¨**ã€‚è¯·æŒ‰ä»¥ä¸‹è§„èŒƒç”Ÿæˆï¼š

1.  **è¡¨æ ¼**:
    -   ä½¿ç”¨ Markdown è¡¨æ ¼è¯­æ³•ã€‚
    -   **è¡¨å**: åœ¨è¡¨æ ¼**ä¸Šæ–¹**ï¼Œæ ¼å¼ï¼š`**è¡¨{chapter_num}.X è¡¨å**`ã€‚
2.  **ç»Ÿè®¡å›¾ (Python Matplotlib)**:
    -   è¯·ç¼–å†™ä¸€æ®µ**æ ‡å‡†ã€æ— é”™ã€å¯ç›´æ¥è¿è¡Œçš„ Python ä»£ç **ã€‚
    -   **ä»£ç å—æ ¼å¼**: ä½¿ç”¨ ` ```python ` åŒ…è£¹ã€‚
    -   **å…³é”®è¦æ±‚ (CRITICAL)**: 
        -   **åº“å¯¼å…¥**: å¿…é¡»åœ¨ä»£ç å¼€å¤´æ˜¾å¼å¯¼å…¥ï¼š`import matplotlib.pyplot as plt`, `import seaborn as sns`, `import pandas as pd`, `import numpy as np`ã€‚
        -   **æ•°æ®è‡ªåŒ…å«**: æ•°æ®å¿…é¡»åœ¨ä»£ç å†…éƒ¨å®Œæ•´å®šä¹‰ï¼ˆä½¿ç”¨ DataFrame æˆ–å­—å…¸ï¼‰ï¼Œ**ä¸¥ç¦**è¯»å–å¤–éƒ¨æ–‡ä»¶ã€‚
        -   **æ ¼å¼è§„èŒƒ**: ä¸¥ç¦ä½¿ç”¨å…¨è§’ç©ºæ ¼ï¼ˆ\\u3000ï¼‰æˆ–ä¸é—´æ–­ç©ºæ ¼ï¼ˆNBSPï¼‰ï¼Œå¿…é¡»ä½¿ç”¨æ ‡å‡†ç©ºæ ¼ç¼©è¿›ã€‚
        -   **å­—ä½“è®¾ç½®**: å¿…é¡»åŒ…å« `plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'sans-serif']` è§£å†³ä¸­æ–‡ä¹±ç ã€‚
        -   **ç»˜å›¾é€»è¾‘**: ä»£ç éœ€ç®€å•å¥å£®ï¼Œä¸è¦ä½¿ç”¨å¤æ‚æˆ–è¿‡æ—¶çš„ APIã€‚
        -   **è¾“å‡º**: æœ€å**ä¸éœ€è¦** `plt.show()`ã€‚
    -   **å›¾å**: åœ¨ä»£ç å—**ä¸‹æ–¹**ï¼Œæ ¼å¼ï¼š`**å›¾{chapter_num}.X å›¾å**`ã€‚
3.  **äº’åŠ¨**: æ­£æ–‡å¿…é¡»åŒ…å« â€œå¦‚è¡¨{chapter_num}.1æ‰€ç¤ºâ€ æˆ– â€œå¦‚å›¾{chapter_num}.1å¯è§â€ã€‚
"""
    else:
        visuals_instruction = "### **ç­–ç•¥F: å›¾è¡¨ç¦ä»¤**\n**ä¸¥ç¦ç”Ÿæˆä»»ä½•å›¾è¡¨ã€‚**"

    # ----------------- æœ€ç»ˆç»„åˆ (ç­–ç•¥A-Eä¿æŒåŸæ ·) -----------------
    return f"""
# è§’è‰²
ä½ ç°åœ¨æ‰®æ¼”ä¸€ä½**ä¸¥è°¨çš„å­¦æœ¯å¯¼å¸ˆ**ï¼Œè¾…åŠ©å­¦ç”Ÿæ’°å†™æ¯•ä¸šè®ºæ–‡ã€‚
ä»»åŠ¡ï¼šä¸¥æ ¼éµå¾ªç‰¹å®šçš„å†™ä½œæ¨¡æ¿ï¼Œä¿è¯å­¦æœ¯è§„èŒƒï¼Œ**ç»ä¸å¤¸å¤§æˆæœ**ï¼Œ**å›¾æ–‡å¹¶èŒ‚**ã€‚

### **ç­–ç•¥A: æ ¼å¼ä¸æ’ç‰ˆ**
1.  **æ®µè½ç¼©è¿›**: **æ‰€æœ‰æ®µè½å¼€å¤´å¿…é¡»åŒ…å«ä¸¤ä¸ªå…¨è§’ç©ºæ ¼ï¼ˆã€€ã€€ï¼‰**ã€‚
2.  **ç¦ç”¨åˆ—è¡¨**: ä¸¥ç¦ä½¿ç”¨ Markdown åˆ—è¡¨ï¼Œå¿…é¡»å†™æˆè¿è´¯æ®µè½ï¼ˆç ”ç©¶æ–¹æ³•é™¤å¤–ï¼‰ã€‚


### **ç­–ç•¥B: æ•°æ®ä¸è°¦æŠ‘æ€§ (CRITICAL)**
1.  **å­—ä½“è§„èŒƒ**: **æ‰€æœ‰æ•°å­—ã€å­—æ¯ã€æ ‡ç‚¹å¿…é¡»ä½¿ç”¨åŠè§’å­—ç¬¦ (Half-width)**ã€‚
    -   æ­£ç¡®: 2023, 50%, "Method"
    -   é”™è¯¯: ï¼’ï¼ï¼’ï¼“, ï¼•ï¼ï¼…, â€œMethodâ€
2.  **æ•°æ®ä¼˜å…ˆçº§**: 
    -   **æœ€é«˜ä¼˜å…ˆçº§**: å¦‚æœè¾“å…¥ä¸­åŒ…å«ã€ç”¨æˆ·æä¾›çš„çœŸå®æ•°æ®ã€‘ï¼Œå¿…é¡»**æ— æ¡ä»¶åŸºäºè¯¥æ•°æ®**è¿›è¡Œåˆ†æä¸åˆ¶å›¾ï¼Œ**ä¸¥ç¦ç¯¡æ”¹æ•°å€¼**ã€‚
    -   **æ¬¡çº§æ¥æº**: ä»…åœ¨ç”¨æˆ·æœªæä¾›æ•°æ®æ—¶ï¼Œæ‰ä½¿ç”¨ã€è”ç½‘æ£€ç´¢äº‹å®ã€‘æˆ–é€šç”¨å­¦æœ¯çŸ¥è¯†ã€‚
3.  **ä¸¥ç¦å¤¸å¤§**: 
    -   **ç¦æ­¢**: â€œå¡«è¡¥ç©ºç™½â€ã€â€œå›½å†…é¦–åˆ›â€ã€â€œå®Œç¾è§£å†³â€ã€‚
    -   **å¿…é¡»ç”¨**: â€œä¸°å¯Œäº†...è§†è§’â€ã€â€œæä¾›äº†å®è¯å‚è€ƒâ€ã€â€œä¼˜åŒ–äº†...â€ã€‚
4.  **ä¸¥ç¦æé€ **: æ— è®ºæ˜¯ç”¨æˆ·æ•°æ®è¿˜æ˜¯æ£€ç´¢æ•°æ®ï¼Œéƒ½å¿…é¡»ä¿æŒé€»è¾‘è‡ªæ´½ï¼Œä¸¥ç¦å‡­ç©ºæœæ’°å®éªŒç»“æœã€‚
5.  **æ–‡ä»¶å¼•ç”¨**: **ä¸¥ç¦ç¼–é€ ã€Šã€‹å†…çš„æ”¿ç­–/æ–‡ä»¶/è‘—ä½œåç§°**ã€‚å¿…é¡»ç¡®ä¿è¯¥æ”¿ç­–/æ–‡ä»¶/è‘—ä½œï¼Œåœ¨çœŸå®ä¸–ç•Œå­˜åœ¨ä¸”åç§°å®Œå…¨å‡†ç¡®ã€‚å¦‚æœä¸ç¡®å®šçœŸå®å…¨ç§°ï¼Œ**ä¸¥ç¦ä½¿ç”¨ä¹¦åå·**ï¼Œä»…æè¿°å…¶å†…å®¹å³å¯ã€‚


### **ç­–ç•¥C: ç« èŠ‚ä¸“å±é€»è¾‘**
{section_rule}

{ref_instruction}

{visuals_instruction}

### **ç­–ç•¥E: å­—æ•°æ§åˆ¶**
{word_count_strategy}
**æ‰©å†™æŠ€å·§**: å¦‚æœå­—æ•°ä¸è¶³ï¼Œè¯·å¯¹æ ¸å¿ƒæ¦‚å¿µè¿›è¡Œå®šä¹‰æ‰©å±•ï¼Œæˆ–å¢åŠ â€œä¸¾ä¾‹è¯´æ˜â€ã€â€œå¯¹æ¯”åˆ†æâ€ã€â€œç†è®ºæ”¯æ’‘â€ç­‰ç¯èŠ‚ï¼Œ**ä¸¥ç¦**é€šè¿‡é‡å¤åºŸè¯å‡‘å­—æ•°ã€‚

### **ç­–ç•¥G: ç»“æ„ä¸è¾¹ç•Œæ§åˆ¶ (CRITICAL - ç»å¯¹ç¦æ­¢é¡¹)**
1.  **ç¦æ­¢è‡ªæ‹Ÿæ ‡é¢˜**: è¾“å‡ºå†…å®¹**ä¸¥ç¦åŒ…å«**ä»»ä½• Markdown æ ‡é¢˜ç¬¦å·ï¼ˆ#ã€##ã€###ï¼‰ã€‚
    -   é”™è¯¯ç¤ºä¾‹ï¼š`### 1.1 èƒŒæ™¯åˆ†æ`
    -   æ­£ç¡®æ“ä½œï¼šç›´æ¥å¼€å§‹å†™èƒŒæ™¯åˆ†æçš„**æ­£æ–‡æ®µè½**ã€‚
2.  **ç¦æ­¢è¶Šç•Œ**: **ä¸¥ç¦**æ’°å†™ä¸‹ä¸€ä¸ªç« èŠ‚çš„å†…å®¹ã€‚åªå…³æ³¨å½“å‰ç« èŠ‚ï¼š**â€œ{current_chapter_title}â€**ã€‚
3.  **ç¦æ­¢åˆ†ç‚¹**: é™¤éæ˜¯â€œç ”ç©¶æ–¹æ³•â€ç« èŠ‚ï¼Œå¦åˆ™ä¸¥ç¦ä½¿ç”¨ `1.` `2.` æˆ– `*` è¿›è¡Œç½—åˆ—ï¼Œè¯·ç”¨â€œé¦–å…ˆã€å…¶æ¬¡ã€æ­¤å¤–â€ç­‰è¿æ¥è¯å†™æˆè¿è´¯æ®µè½ã€‚

è¯·å¼€å§‹å†™ä½œã€‚
"""



class PaperAutoWriter:
    def __init__(self, api_key: str, base_url: str, model: str):
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model

    def _call_llm(self, system_prompt: str, user_prompt: str) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
                temperature=0.7, stream=False
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"[Error: {str(e)}]"

    def _research_phase(self, topic: str) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä¸¥è°¨æ•°æ®åˆ†æå¸ˆã€‚åˆ—å‡ºå…³äºä¸»é¢˜çš„çœŸå®æ•°æ®ã€æ”¿ç­–ã€‚ä½¿ç”¨åŠè§’æ•°å­—ã€‚"},
                    {"role": "user", "content": f"æ£€ç´¢å…³äº'{topic}'çš„çœŸå®äº‹å®ï¼š"}
                ],
                temperature=0.3, stream=False
            )
            return response.choices[0].message.content.strip()
        except: 
            return ""

    def _extract_chapter_num(self, title: str) -> str:
        match_digit = re.match(r'^(\d+)', title.strip())
        if match_digit: return match_digit.group(1)
        match_cn = re.match(r'^ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ç« |éƒ¨åˆ†]', title.strip())
        if match_cn:
            cn_map = {'ä¸€':'1','äºŒ':'2','ä¸‰':'3','å››':'4','äº”':'5','å…­':'6','ä¸ƒ':'7','å…«':'8','ä¹':'9','å':'10'}
            return cn_map.get(match_cn.group(1), "")
        return ""

    def generate_stream(self, task_id: str, title: str, chapters: List[Dict], references_raw: str, custom_data: str, check_status_func, initial_context: str = "") -> Generator[str, None, None]:
        ref_manager = ReferenceManager(references_raw)
        yield f"data: {json.dumps({'type': 'log', 'msg': 'åˆå§‹åŒ–...'})}\n\n"
        chapter_ref_map = ref_manager.distribute_references_smart(chapters)
        
        full_content = f"# {title}\n\n"
        context = initial_context if initial_context else "è®ºæ–‡å¼€å¤´"
        
        for i, chapter in enumerate(chapters):
            while check_status_func() == "paused":
                time.sleep(1)
            if check_status_func() == "stopped": 
                yield f"data: {json.dumps({'type': 'log', 'msg': 'âš ï¸ æ”¶åˆ°åœæ­¢æŒ‡ä»¤ï¼Œæ­£åœ¨ä¸­æ–­...'})}\n\n"
                break
            sec_title = chapter['title']
            if chapter.get('is_parent', False):
                full_content += f"## {sec_title}\n\n"
                md_content = f"## {sec_title}\n\n"
                yield f"data: {json.dumps({'type': 'content', 'md': md_content})}\n\n"
                continue

            target = int(chapter.get('words', 500))
            assigned_refs = chapter_ref_map.get(i, [])
            ref_manager.set_current_chapter_refs(assigned_refs)
            chapter_num = self._extract_chapter_num(sec_title)
            
            yield f"data: {json.dumps({'type': 'log', 'msg': f'æ­£åœ¨æ’°å†™: {sec_title}'})}\n\n"
            
            facts_context = ""
            if "æ‘˜è¦" not in sec_title and "ç»“è®º" not in sec_title:
                if custom_data and len(custom_data.strip()) > 5:
                    # åœºæ™¯1: ç”¨æˆ·æä¾›äº†æ•°æ® -> å¼ºåˆ¶ä½¿ç”¨
                    yield f"data: {json.dumps({'type': 'log', 'msg': f'   - æ­£åœ¨æŒ‚è½½ç”¨æˆ·æä¾›çš„çœŸå®æ•°æ®...'})}\n\n"
                    # å¯¹ç”¨æˆ·æ•°æ®ä¹Ÿåšç®€å•çš„å…¨è§’è½¬åŠè§’æ¸…æ´—
                    cleaned_data = TextCleaner.convert_cn_numbers(custom_data)
                    facts_context = f"\nã€ç”¨æˆ·æä¾›çš„çœŸå®æ•°æ® (æœ€é«˜ä¼˜å…ˆçº§)ã€‘:\n{cleaned_data}\n\nè¯·ä¸¥æ ¼åŸºäºä»¥ä¸Šæ•°æ®è¿›è¡Œè®ºè¿°å’Œåˆ†æã€‚"
                else:
                    # åœºæ™¯2: ç”¨æˆ·æœªæä¾› -> è”ç½‘/çŸ¥è¯†åº“æ£€ç´¢
                    yield f"data: {json.dumps({'type': 'log', 'msg': f'   - æœªæ£€æµ‹åˆ°ç”¨æˆ·æ•°æ®ï¼Œæ­£åœ¨æ£€ç´¢ç½‘ç»œ/çŸ¥è¯†åº“æ•°æ®...'})}\n\n"
                    facts = self._research_phase(f"{title} - {sec_title}")
                    if facts:
                        facts = TextCleaner.convert_cn_numbers(facts)
                        facts_context = f"\nã€è”ç½‘æ£€ç´¢äº‹å®åº“ã€‘:\n{facts}"

            sys_prompt = get_academic_thesis_prompt(target, [r[1] for r in assigned_refs], sec_title, chapter_num)
            user_prompt = f"é¢˜ç›®ï¼š{title}\nç« èŠ‚ï¼š{sec_title}\nå‰æ–‡ï¼š{context[-600:]}\nå­—æ•°ï¼š{target}\n{facts_context}"
            
            raw_content = self._call_llm(sys_prompt, user_prompt)

            # --- [æ–°å¢] æ™ºèƒ½å­—æ•°æ£€æŸ¥ä¸æ‰©å†™é€»è¾‘ ---
            # ç»Ÿè®¡æœ‰æ•ˆå­—ç¬¦æ•° (å»é™¤ç©ºç™½ç¬¦)
            current_len = len(re.sub(r'\s', '', raw_content))
            # å¦‚æœå­—æ•°å°‘äºç›®æ ‡çš„ 60% (æ ¹æ®å®é™…ä½“éªŒè°ƒæ•´é˜ˆå€¼)ï¼Œå¼ºåˆ¶æ‰©å†™
            if current_len < target * 0.5:
                yield f"data: {json.dumps({'type': 'log', 'msg': f'   - æ£€æµ‹åˆ°å­—æ•°ä¸è¶³ ({current_len}/{target})ï¼Œè¿›è¡Œæ·±åº¦æ‰©å†™(ç¦æ­¢æ–°å¢æ ‡é¢˜)...'})}\n\n"
                
                expand_prompt = (
                    f"å½“å‰è¾“å‡ºå­—æ•° ({current_len}å­—) æœªè¾¾åˆ°è¦æ±‚ ({target}å­—)ã€‚\n"
                    f"è¯·å¯¹å†…å®¹è¿›è¡Œ**æ·±åº¦æ‰©å†™**ï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹çº¢çº¿ï¼š\n"
                    f"1. **é›¶åºŸè¯**ï¼šä¸¥ç¦è¾“å‡ºâ€œå¥½çš„â€ã€â€œå¦‚ä¸‹æ‰€ç¤ºâ€ã€â€œç»è¿‡æ‰©å†™â€ç­‰ä»»ä½•å¯¹è¯å¼è¯­å¥ã€‚**ç›´æ¥è¾“å‡ºè®ºæ–‡æ­£æ–‡**ã€‚\n"
                    f"2. **å¼•ç”¨ä¿æŠ¤**ï¼šæ‰€æœ‰å‚è€ƒæ–‡çŒ®å¼•ç”¨ï¼ˆä½œè€…ã€å¹´ä»½ã€REFæ ‡è®°ï¼‰å¿…é¡»**åŸæ ·ä¿ç•™**ï¼Œä¸å¯ä¿®æ”¹ã€‚\n"
                    f"3. **ç¦æ­¢æ ‡é¢˜**ï¼šä¸¥ç¦æ–°å¢ä»»ä½• Markdown æ ‡é¢˜ (#/##)ã€‚\n"
                    f"4. **ç¦æ­¢ç»Ÿè®¡**ï¼šæ–‡æœ«ä¸¥ç¦è¾“å‡ºâ€œ(å…¨æ–‡xå­—)â€ä¹‹ç±»çš„ç»Ÿè®¡è¯­ã€‚\n"
                    f"è¯·ç›´æ¥è¾“å‡ºæ‰©å†™åçš„å†…å®¹ã€‚"
                )
                
                # å°†åˆç¨¿ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œè¦æ±‚æ‰©å†™
                expand_messages = [
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_prompt},
                    {"role": "assistant", "content": raw_content},
                    {"role": "user", "content": expand_prompt}
                ]
                try:
                    # ä½¿ç”¨ä¸´æ—¶è°ƒç”¨è¿›è¡Œæ‰©å†™
                    resp = self.client.chat.completions.create(
                        model=self.model,
                        messages=expand_messages,
                        temperature=0.7
                    )
                    raw_content = resp.choices[0].message.content.strip()
                except Exception as e:
                    print(f"æ‰©å†™å¤±è´¥: {e}")

            # 2. [æ–°å¢] å­—æ•°è¿‡å¤š -> ç²¾ç®€
            elif current_len > target * 1.5:
                yield f"data: {json.dumps({'type': 'log', 'msg': f'   - æ£€æµ‹åˆ°å­—æ•°è¿‡å¤š ({current_len}/{target})ï¼Œæ­£åœ¨è¿›è¡Œç²¾ç®€...'})}\n\n"
                
                condense_prompt = (
                    f"å½“å‰è¾“å‡ºå­—æ•° ({current_len}å­—) è¿œè¶…è¦æ±‚ ({target}å­—)ã€‚\n"
                    f"è¯·å¯¹å†…å®¹è¿›è¡Œ**ç²¾ç®€**ï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹çº¢çº¿ï¼š\n"
                    f"1. **é›¶åºŸè¯**ï¼šä¸¥ç¦è¾“å‡ºâ€œå¥½çš„â€ã€â€œå·²ä¸ºæ‚¨ç²¾ç®€â€ç­‰ä»»ä½•å¯¹è¯å¼è¯­å¥ã€‚**ç›´æ¥è¾“å‡ºè®ºæ–‡æ­£æ–‡**ã€‚\n"
                    f"2. **å¼•ç”¨ç»å¯¹ä¿ç•™**ï¼šåŸæ–‡ä¸­çš„æ‰€æœ‰å‚è€ƒæ–‡çŒ®å¼•ç”¨ï¼ˆå¦‚ 'ä½œè€…(å¹´ä»½)...[REF]'ï¼‰**å¿…é¡»åŸæ ·ä¿ç•™**ï¼Œä¸¥ç¦åˆ é™¤æˆ–ä¿®æ”¹ã€‚\n"
                    f"3. **ç¦æ­¢ç»Ÿè®¡**ï¼šæ–‡æœ«ä¸¥ç¦è¾“å‡ºâ€œ(å…¨æ–‡xå­—)â€ä¹‹ç±»çš„ç»Ÿè®¡è¯­ã€‚\n"
                    f"4. **ä¸¥æ§ç¯‡å¹…**ï¼šç›®æ ‡å­—æ•° {target} å­—å·¦å³ã€‚\n"
                    f"è¯·ç›´æ¥è¾“å‡ºç²¾ç®€åçš„å†…å®¹ã€‚"
                )
                
                condense_messages = [
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_prompt},
                    {"role": "assistant", "content": raw_content},
                    {"role": "user", "content": condense_prompt}
                ]
                
                try:
                    resp = self.client.chat.completions.create(
                        model=self.model,
                        messages=condense_messages,
                        temperature=0.7
                    )
                    raw_content = resp.choices[0].message.content.strip()
                except Exception as e:
                    print(f"ç²¾ç®€å¤±è´¥: {e}")


            # æ–¹æ¡ˆï¼šæ¯”å¯¹ç¬¬ä¸€è¡Œå’Œsec_titleï¼Œå¦‚æœé«˜åº¦ç›¸ä¼¼åˆ™ç§»é™¤ç¬¬ä¸€è¡Œ
            temp_lines = raw_content.strip().split('\n')
            if temp_lines:
                # å»é™¤ # * å’Œç©ºæ ¼è¿›è¡Œæ ¸å¿ƒè¯æ¯”å¯¹
                first_line_core = re.sub(r'[#*\s]', '', temp_lines[0])
                title_core = re.sub(r'[#*\s]', '', sec_title)
                # å¦‚æœç¬¬ä¸€è¡ŒåŒ…å«æ ‡é¢˜æ ¸å¿ƒå†…å®¹ï¼Œä¸”é•¿åº¦æ²¡æœ‰æ¯”æ ‡é¢˜é•¿å¤ªå¤šï¼ˆé˜²æ­¢è¯¯åˆ æ­£æ–‡ç¬¬ä¸€å¥ï¼‰ï¼Œåˆ™åˆ¤å®šä¸ºé‡å¤æ ‡é¢˜
                if title_core in first_line_core and len(first_line_core) < len(title_core) + 8:
                    raw_content = '\n'.join(temp_lines[1:])
            processed_content = ref_manager.process_text_deterministic(raw_content)
            processed_content = TextCleaner.convert_cn_numbers(processed_content)
            
            lines = processed_content.split('\n')
            formatted_lines = []
            for line in lines:
                line = line.strip()
                if (line and not line.startswith('ã€€ã€€') and not line.startswith('#') and 
                    not line.startswith('|') and not line.startswith('```') and "import" not in line and "plt." not in line):
                    line = 'ã€€ã€€' + line 
                formatted_lines.append(line)
            final_content = '\n\n'.join(formatted_lines)

            section_md = f"## {sec_title}\n\n{final_content}\n\n"
            full_content += section_md
            context = final_content
            yield f"data: {json.dumps({'type': 'content', 'md': section_md})}\n\n"

        if check_status_func() != "stopped":
            bib = ref_manager.generate_bibliography()
            full_content += bib
            yield f"data: {json.dumps({'type': 'content', 'md': bib})}\n\n"
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
        else:
            yield f"data: {json.dumps({'type': 'log', 'msg': 'ğŸ›‘ ä»»åŠ¡å·²å®Œå…¨ç»ˆæ­¢ (å·²è·³è¿‡åç»­å†…å®¹)'})}\n\n"