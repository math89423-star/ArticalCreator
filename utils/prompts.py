import re
import json
import time
from typing import List
from openai import OpenAI
from typing import Dict, Generator
from .reference import ReferenceManager
from .word import TextCleaner


def get_academic_thesis_prompt(target_words: int, ref_content_list: List[str], current_chapter_title: str, chapter_num: str) -> str:
    
    # ------------------------------------------------------------------
    # 1. ç« èŠ‚ä¸“å±é€»è¾‘
    # ------------------------------------------------------------------
    section_rule = ""
    is_cn_abstract = "æ‘˜è¦" in current_chapter_title
    is_en_abstract = "Abstract" in current_chapter_title and "æ‘˜è¦" not in current_chapter_title

    
    # åˆ¤æ–­æ˜¯å¦ä¸ºéœ€è¦å›¾è¡¨çš„ç« èŠ‚ (æ ¸å¿ƒä¿®å¤)
    # åªæœ‰åŒ…å«ä»¥ä¸‹å…³é”®è¯çš„ç« èŠ‚æ‰å¯ç”¨å›¾è¡¨
    needs_charts = False
    if chapter_num and any(k in current_chapter_title for k in ["å®éªŒ", "æµ‹è¯•", "åˆ†æ", "ç»“æœ", "æ•°æ®", "è®¾è®¡", "å®ç°", "éªŒè¯", "Evaluation", "Analysis", "Design"]):
        needs_charts = True
    
    # A. æ‘˜è¦
    if is_cn_abstract:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™æ‘˜è¦ä¸å…³é”®è¯**
**çº¢çº¿è§„åˆ™**: 
1. **ä¸¥ç¦è¾“å‡ºæ ‡é¢˜**: ä¸è¦è¾“å‡º "### æ‘˜è¦" æˆ– "### Abstract"ï¼Œç›´æ¥å¼€å§‹å†™æ­£æ–‡ã€‚
2. **ä¸­è‹±å¯¹ç…§**: å…ˆå†™ä¸­æ–‡éƒ¨åˆ†ï¼Œå†å†™è‹±æ–‡éƒ¨åˆ†ã€‚è‹¥æ²¡æœ‰è‹±æ–‡ï¼ˆAbstractï¼‰éƒ¨åˆ†åˆ™ä¸å†™è‹±æ–‡éƒ¨åˆ†ã€‚

**é€»è¾‘ç»“æ„**:
1. **ç ”ç©¶èƒŒæ™¯**: ç®€è¿°èƒŒæ™¯ï¼ˆçº¦50å­—ï¼‰ã€‚
2. **æ–¹æ³•åˆ›æ–°**: åšäº†ä»€ä¹ˆï¼Œç”¨äº†ä»€ä¹ˆæ–¹æ³•ï¼ˆçº¦100å­—ï¼‰ã€‚
3. **å…³é”®å‘ç°**: å¾—åˆ°äº†ä»€ä¹ˆæ•°æ®æˆ–ç»“è®ºï¼ˆçº¦100å­—ï¼‰ã€‚
4. **ç†è®ºè´¡çŒ®**: ä»·å€¼æ˜¯ä»€ä¹ˆï¼ˆçº¦50å­—ï¼‰ã€‚

**æ ¼å¼æ¨¡æ¿**:
   - ç›´æ¥è¾“å‡ºæ‘˜è¦æ­£æ–‡ã€‚
   - æœ€åä¸€è¡Œè¾“å‡ºï¼š**å…³é”®è¯**ï¼šè¯1ï¼›è¯2ï¼›è¯3
"""

    elif is_en_abstract:
        section_rule = """
**Current Task: Write English Abstract & Keywords**
**Requirements**:
1. **Language**: MUST be written in **English** only. No Chinese allowed.
2. **Content**: Translate the logic of a standard academic abstract (Background -> Method -> Results -> Conclusion).
3. **Format**: 
   - Output the abstract body directly.
   - Last line: **Keywords**: Word1; Word2; Word3
"""

    # B. èƒŒæ™¯ä¸æ„ä¹‰
    elif "èƒŒæ™¯" in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶èƒŒæ™¯**
**è¦æ±‚**:
1. **çœŸå®æ”¿ç­–**: å¿…é¡»ç»“åˆè¿‘å‡ å¹´ä¸­å›½çœŸå®å­˜åœ¨çš„å›½å®¶æ”¿ç­–ã€æœ€æ–°æ–‡ä»¶ã€é‡å¤§ç›¸å…³äº‹é¡¹ã€‚
2. **æ•°æ®æ”¯æ’‘**: éœ€è¦ä¸€ç‚¹çœŸå®æ•°æ®ä½œä¸ºèƒŒæ™¯æ”¯æ’‘ã€‚
3. **ç¯‡å¹…**: 350å­—å·¦å³ï¼Œä¸æ³›æ³›è€Œè°ˆã€‚
"""
    elif "æ„ä¹‰" in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶æ„ä¹‰**
**è¦æ±‚**:
1. **ç†è®ºæ„ä¹‰**: ä¸¥ç¦è¯´â€œå¡«è¡¥äº†ç©ºç™½â€ï¼Œå¿…é¡»è¯´â€œ**ä¸°å¯Œäº†...ç†è®ºæ¡†æ¶**â€æˆ–â€œ**ä¸º...æä¾›äº†å®è¯è¡¥å……**â€ã€‚
2. **å®é™…æ„ä¹‰**: è§£å†³å…·ä½“è¡Œä¸šæˆ–ç¤¾ä¼šç—›ç‚¹ã€‚
3. **ç¯‡å¹…**: 350å­—å·¦å³ã€‚
"""

# C. å›½å†…å¤–ç ”ç©¶ç°çŠ¶
    elif any(k in current_chapter_title for k in ["ç°çŠ¶", "ç»¼è¿°", "Review"]):
        if ref_content_list:
            first_ref = ref_content_list[0]
            # Pythoné€»è¾‘ä¿®å¤ï¼šå¿…é¡»ä¼ å…¥çœŸå®å†…å®¹ï¼Œè€ŒéIDå ä½ç¬¦
            if len(ref_content_list) > 1:
                other_refs_prompt = "\n".join([f"{{æ–‡çŒ®{i+2}}}: {ref}" for i, ref in enumerate(ref_content_list[1:])])
            else:
                other_refs_prompt = "æ— åç»­æ–‡çŒ®"
            
            section_rule = f"""
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶ç°çŠ¶ (æ–‡çŒ®ç»¼è¿°)**
**æ ¸å¿ƒç›®æ ‡**ï¼šå°†æä¾›çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨è½¬åŒ–ä¸ºé€»è¾‘é€šé¡ºçš„å­¦æœ¯è¯„è¿°ã€‚

### **å¼•ç”¨è§„èŒƒ (é›¶å®¹å¿è§„åˆ™)**
1.  **ç¦æ­¢å‡ºç°ID**: æ­£æ–‡ä¸­**ç»å¯¹ç¦æ­¢**å‡ºç° "å‚è€ƒæ–‡çŒ®ID"ã€"æ–‡çŒ®1"ã€"Reference ID" ç­‰å­—æ ·ã€‚
2.  **å¼•ç”¨æ ¼å¼**: å¿…é¡»ä»æ–‡çŒ®å†…å®¹ä¸­æå–**ä½œè€…**å’Œ**å¹´ä»½**ï¼Œæ ¼å¼ä¸º `ä½œè€…(å¹´ä»½)`ã€‚
    -   *ä¾‹*: "Zhang (2023) æå‡ºäº†..." æˆ– "OpenAI (2024) å‘å¸ƒäº†..."
    -   *å¦‚æœæ‰¾ä¸åˆ°ä½œè€…*: ä½¿ç”¨ `ã€Šæ ‡é¢˜ã€‹(å¹´ä»½)`ã€‚
3.  **ç¦æ­¢æ¨¡ç³Š**: ä¸¥ç¦ä½¿ç”¨ "æŸå­¦è€…"ã€"æœ‰ç ”ç©¶"ã€"è¯¥ä½œå“" ç­‰æŒ‡ä»£ä¸æ˜çš„è¯ï¼Œ**å¿…é¡»æŒ‡åé“å§“**ã€‚
4.  **é¡ºåºä¸é¢‘æ¬¡**: 
    -   å¿…é¡»**ä¸¥æ ¼æŒ‰ç…§åˆ—è¡¨é¡ºåº**é€ä¸€è®ºè¿°ã€‚
    -   åˆ—è¡¨ä¸­çš„**æ¯ä¸€æ¡**æ–‡çŒ®éƒ½å¿…é¡»è¢«å¼•ç”¨**ä¸€æ¬¡ä¸”ä»…ä¸€æ¬¡**ã€‚
    -   æ¯æ®µè®ºè¿°ç»“æŸå¥æœ«å°¾å¿…é¡»åŠ  `[REF]` æ ‡è®°ã€‚

### **å†™ä½œé€»è¾‘**
1.  **ç¬¬ä¸€æ®µ (å¯¼è¯­)**: ç®€è¦æ¦‚æ‹¬è¯¥é¢†åŸŸçš„æ€»ä½“å‘å±•è¶‹åŠ¿ï¼ˆçº¦80å­—ï¼‰ã€‚
2.  **ç¬¬äºŒæ®µ (æ ¸å¿ƒç»¼è¿°)**: 
    -   **é¦–æ¡è¯¦è¿°**: é’ˆå¯¹ **{{æ–‡çŒ®1}}** ({first_ref}) è¿›è¡Œè¯¦ç»†è¯„è¿°ï¼ˆçº¦150å­—ï¼‰ã€‚å†™æ˜ï¼šä½œè€…+å¹´ä»½+æ ¸å¿ƒè´¡çŒ®+å±€é™æ€§ã€‚æ–‡æœ«åŠ  `[REF]`ã€‚
    -   **åç»­ä¸²è”**: ä¾æ¬¡å¯¹ **{{æ–‡çŒ®2}}** åŠåç»­æ–‡çŒ®è¿›è¡Œè¯„è¿°ã€‚
        -   *å¿…é¡»ä»æä¾›çš„æ–‡æœ¬ä¸­æå–çœŸå®ä½œè€…å’Œè§‚ç‚¹ï¼Œä¸¥ç¦ç¼–é€ ã€‚*
        -   ä½¿ç”¨è¿æ¥è¯ï¼ˆå¦‚"ä¸ä¹‹ç±»ä¼¼"ã€"ç„¶è€Œ"ã€"åœ¨æ­¤åŸºç¡€ä¸Š"ï¼‰å°†ä¸åŒæ–‡çŒ®é€»è¾‘ä¸²è”ã€‚
        -   æ ¼å¼ï¼š`ä½œè€…(å¹´ä»½) + è§‚ç‚¹/æ–¹æ³• + [REF]`ã€‚
3.  **ç¬¬ä¸‰æ®µ (è¯„è¿°)**: æ€»ç»“ä¸Šè¿°æ–‡çŒ®çš„å…±åŒä¸è¶³ï¼Œå¼•å‡ºæœ¬ç ”ç©¶çš„åˆ‡å…¥ç‚¹ã€‚

**å¾…ç»¼è¿°çš„æ–‡çŒ®åˆ—è¡¨ (è¯·ä»ä¸­æå–ä¿¡æ¯)**:
- {{æ–‡çŒ®1}}: {first_ref}
{other_refs_prompt}
"""
        else:
            section_rule = "**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶ç°çŠ¶**\nè¯·åŸºäºé€šç”¨å­¦æœ¯çŸ¥è¯†æ’°å†™ï¼Œä¿æŒæ€»åˆ†æ€»ç»“æ„ï¼Œå¼•ç”¨çœŸå®å­˜åœ¨çš„ç»å…¸æ–‡çŒ®ã€‚"

    # D. æ–‡çŒ®è¿°è¯„
    elif "è¿°è¯„" in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™æ–‡çŒ®è¿°è¯„**
**è¦æ±‚**: 
1. **ä¸å¼•ç”¨**: æ­¤éƒ¨åˆ†ä¸éœ€è¦å¼•ç”¨å…·ä½“æ–‡çŒ®ã€‚
2. **å†…å®¹**: æ€»ç»“å‰æ–‡æ–‡çŒ®çš„ä¸è¶³ï¼ŒæŒ‡å‡ºæœ¬ç ”ç©¶çš„åˆ‡å…¥ç‚¹ï¼ˆå€Ÿé‰´ä»€ä¹ˆï¼Œä¸°å¯Œä»€ä¹ˆï¼‰ã€‚
3. **ç¯‡å¹…**: ä¸€ä¸ªæ®µè½ï¼Œ300å­—å·¦å³ã€‚
"""

    # E. ç ”ç©¶å†…å®¹
    elif "ç ”ç©¶å†…å®¹" in current_chapter_title and "æ–¹æ³•" not in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶å†…å®¹**
**æ ¼å¼**: åˆ†æ®µå¼å›ç­”ã€‚
1. **å¯¼è¯­**: â€œæœ¬ç ”ç©¶ä¸»è¦ç ”ç©¶...ï¼Œå…·ä½“å†…å®¹å¦‚ä¸‹ï¼šâ€
2. **åˆ†ç« èŠ‚**: 
   - â€œç¬¬ä¸€éƒ¨åˆ†ï¼Œç»ªè®ºã€‚ä¸»è¦é˜è¿°...â€
   - â€œç¬¬äºŒéƒ¨åˆ†ï¼Œ...ã€‚åˆ†æäº†...â€
   - ...
**è¦æ±‚**: æ ¸å¿ƒç« èŠ‚è§£é‡Šçº¦200å­—ï¼Œè¯¦ç•¥å¾—å½“ã€‚
"""

    # F. ç ”ç©¶æ–¹æ³•
    elif "ç ”ç©¶æ–¹æ³•" in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶æ–¹æ³•**
**æ ¼å¼**: åˆ†ç‚¹å›ç­”ï¼Œå¿…é¡»æ ‡åºå· (1. 2. 3.)ã€‚
**å¿…é€‰æ–¹æ³• (æŒ‰éœ€é€‰æ‹©)**:
1. **æ–‡çŒ®ç ”ç©¶æ³•**: (å¦‚æœ‰å‚è€ƒæ–‡çŒ®åˆ™å¿…é€‰)
2. **æ•°æ®åˆ†ææ³•**: (å¦‚æœ‰æ•°æ®åˆ†æåˆ™å¿…é€‰)
3. **å®è¯ç ”ç©¶æ³•/æ¡ˆä¾‹åˆ†ææ³•**: (æ ¹æ®é¢˜ç›®åˆ¤æ–­)
**è¦æ±‚**: ç»“åˆè®ºæ–‡ä¸»é¢˜è§£é‡Šä¸ºä»€ä¹ˆç”¨è¿™ä¸ªæ–¹æ³•ã€‚
"""

    # G. é€šç”¨æ­£æ–‡ (æ–°å¢æ•°æ®åˆ†æè¦æ±‚)
    else:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™æ­£æ–‡åˆ†æ**
1. **é€»è¾‘ä¸»å¯¼**: æ ¸å¿ƒæ˜¯åˆ†ææ€è·¯ã€‚
2. **æ·±åº¦è®ºè¿°**: æ¯ä¸€æ®µéƒ½è¦æœ‰è§‚ç‚¹ã€æœ‰è®ºæ®ï¼ˆæ•°æ®æˆ–ç†è®ºï¼‰ã€æœ‰ç»“è®ºã€‚
"""

    # ------------------------------------------------------------------
    # 2. å¼•ç”¨æŒ‡ä»¤
    # ------------------------------------------------------------------
    ref_instruction = ""
    if ref_content_list and any(k in current_chapter_title for k in ["ç°çŠ¶", "ç»¼è¿°", "Review"]):
        ref_instruction = f"""
### **ç­–ç•¥D: å¼•ç”¨æ‰§è¡Œ (Token Strategy)**
æœ¬ç« èŠ‚å¿…é¡»å¼•ç”¨åˆ†é…çš„ {len(ref_content_list)} æ¡æ–‡çŒ®ã€‚
1.  **ä¸è¦ç”Ÿæˆåºå·**: ä¸è¦å†™ [1] [2]ã€‚
2.  **æ’å…¥æ ‡è®°**: åœ¨æåˆ°æ–‡çŒ®è§‚ç‚¹æ—¶ï¼Œæ’å…¥ **`[REF]`**ã€‚
3.  **æ•°é‡**: å¿…é¡»æ’å…¥ {len(ref_content_list)} ä¸ª `[REF]` æ ‡è®°ã€‚
4.  **å…³è”**: å³ä½¿æ–‡çŒ®ä¸ç›¸å…³ï¼Œä¹Ÿè¦ç”¨â€œæ­¤å¤–ï¼Œä¹Ÿæœ‰ç ”ç©¶æŒ‡å‡º...â€å¼ºè¡Œå…³è”ï¼Œ**è‡ªåœ†å…¶è¯´**ã€‚
"""
    else:
        ref_instruction = "### **ç­–ç•¥D: å¼•ç”¨ç­–ç•¥**\næœ¬ç« èŠ‚æ— éœ€å¼ºåˆ¶å¼•ç”¨åˆ—è¡¨ä¸­çš„æ–‡çŒ®ï¼Œå¦‚éœ€å¼•ç”¨æ•°æ®è¯·ä½¿ç”¨çœŸå®çŸ¥è¯†ã€‚"

    word_count_strategy = f"ç›®æ ‡: **{target_words} å­—**ã€‚" if not (is_en_abstract or is_cn_abstract) else "å­—æ•°ç›®æ ‡ä»…é€‚ç”¨äºä¸­æ–‡éƒ¨åˆ†ã€‚"

    # ----------------- ç­–ç•¥F: æ›´æ–°ä¸º Python ç»˜å›¾ -----------------
    visuals_instruction = ""
    if needs_charts:
        visuals_instruction = f"""
### **ç­–ç•¥F: å›¾è¡¨ä¸æ•°æ®å¯è§†åŒ– (Python & Tables)**
**æœ¬ç« èŠ‚å¿…é¡»åŒ…å«å›¾è¡¨**ã€‚è¯·æŒ‰ä»¥ä¸‹è§„èŒƒç”Ÿæˆï¼š

1.  **è¡¨æ ¼**:
    -   ä½¿ç”¨ Markdown è¡¨æ ¼è¯­æ³•ã€‚
    -   **è¡¨å**: åœ¨è¡¨æ ¼**ä¸Šæ–¹**ï¼Œæ ¼å¼ï¼š`**è¡¨{chapter_num}.X è¡¨å**`ã€‚
2.  **ç»Ÿè®¡å›¾ (Python Matplotlib)**:
    -   è¯·ç¼–å†™ä¸€æ®µ**æ ‡å‡†ã€æ— é”™ã€å¯ç›´æ¥è¿è¡Œçš„ Python ä»£ç **ã€‚
    -   **ä»£ç å—æ ¼å¼**: ä½¿ç”¨ ` ```python ` åŒ…è£¹ã€‚
    -   **å…³é”®è¦æ±‚ (CRITICAL)**: 
        -   **åº“å¯¼å…¥**: å¿…é¡»åœ¨ä»£ç å¼€å¤´æ˜¾å¼å¯¼å…¥ï¼š`import matplotlib.pyplot as plt`, `import seaborn as sns`, `import pandas as pd`, `import numpy as np`ã€‚
        -   **æ•°æ®è‡ªåŒ…å«**: æ•°æ®å¿…é¡»åœ¨ä»£ç å†…éƒ¨å®Œæ•´å®šä¹‰ï¼ˆä½¿ç”¨ DataFrame æˆ–å­—å…¸ï¼‰ï¼Œ**ä¸¥ç¦**è¯»å–å¤–éƒ¨æ–‡ä»¶ã€‚
        -   **æ ¼å¼è§„èŒƒ**: ä¸¥ç¦ä½¿ç”¨å…¨è§’ç©ºæ ¼ï¼ˆ\\u3000ï¼‰æˆ–ä¸é—´æ–­ç©ºæ ¼ï¼ˆNBSPï¼‰ï¼Œå¿…é¡»ä½¿ç”¨æ ‡å‡†ç©ºæ ¼ç¼©è¿›ã€‚
        -   **å­—ä½“è®¾ç½®**: å¿…é¡»åŒ…å« `plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'sans-serif']` è§£å†³ä¸­æ–‡ä¹±ç ã€‚
        -   **ç»˜å›¾é€»è¾‘**: ä»£ç éœ€ç®€å•å¥å£®ï¼Œä¸è¦ä½¿ç”¨å¤æ‚æˆ–è¿‡æ—¶çš„ APIã€‚
        -   **è¾“å‡º**: æœ€å**ä¸éœ€è¦** `plt.show()`ã€‚
    -   **å›¾å**: åœ¨ä»£ç å—**ä¸‹æ–¹**ï¼Œæ ¼å¼ï¼š`**å›¾{chapter_num}.X å›¾å**`ã€‚
3.  **äº’åŠ¨**: æ­£æ–‡å¿…é¡»åŒ…å« â€œå¦‚è¡¨{chapter_num}.1æ‰€ç¤ºâ€ æˆ– â€œå¦‚å›¾{chapter_num}.1å¯è§â€ã€‚
"""
    else:
        visuals_instruction = "### **ç­–ç•¥F: å›¾è¡¨ç¦ä»¤**\n**ä¸¥ç¦ç”Ÿæˆä»»ä½•å›¾è¡¨ã€‚**"

    return f"""
# è§’è‰²
ä½ ç°åœ¨æ‰®æ¼”ä¸€ä½**ä¸¥è°¨çš„å­¦æœ¯å¯¼å¸ˆ**ï¼Œè¾…åŠ©å­¦ç”Ÿæ’°å†™æ¯•ä¸šè®ºæ–‡ã€‚
ä»»åŠ¡ï¼šä¸¥æ ¼éµå¾ªç‰¹å®šçš„å†™ä½œæ¨¡æ¿ï¼Œä¿è¯å­¦æœ¯è§„èŒƒï¼Œ**ç»ä¸å¤¸å¤§æˆæœ**ï¼Œ**å›¾æ–‡å¹¶èŒ‚**ã€‚

### **ç­–ç•¥A: æ ¼å¼ä¸æ’ç‰ˆ**
1.  **æ®µè½ç¼©è¿›**: **æ‰€æœ‰æ®µè½å¼€å¤´å¿…é¡»åŒ…å«ä¸¤ä¸ªå…¨è§’ç©ºæ ¼ï¼ˆã€€ã€€ï¼‰**ã€‚
2.  **ç¦ç”¨åˆ—è¡¨**: ä¸¥ç¦ä½¿ç”¨ Markdown åˆ—è¡¨ï¼Œå¿…é¡»å†™æˆè¿è´¯æ®µè½ï¼ˆç ”ç©¶æ–¹æ³•é™¤å¤–ï¼‰ã€‚


### **ç­–ç•¥B: æ•°æ®ä¸è°¦æŠ‘æ€§ (CRITICAL)**
1.  **å­—ä½“è§„èŒƒ**: **æ‰€æœ‰æ•°å­—ã€å­—æ¯ã€æ ‡ç‚¹å¿…é¡»ä½¿ç”¨åŠè§’å­—ç¬¦ (Half-width)**ã€‚
    -   æ­£ç¡®: 2023, 50%, "Method"
    -   é”™è¯¯: ï¼’ï¼ï¼’ï¼“, ï¼•ï¼ï¼…, â€œMethodâ€
2.  **æ•°æ®ä¼˜å…ˆçº§**: 
    -   **æœ€é«˜ä¼˜å…ˆçº§**: å¦‚æœè¾“å…¥ä¸­åŒ…å«ã€ç”¨æˆ·æä¾›çš„çœŸå®æ•°æ®ã€‘ï¼Œå¿…é¡»**æ— æ¡ä»¶åŸºäºè¯¥æ•°æ®**è¿›è¡Œåˆ†æä¸åˆ¶å›¾ï¼Œ**ä¸¥ç¦ç¯¡æ”¹æ•°å€¼**ã€‚
    -   **æ¬¡çº§æ¥æº**: ä»…åœ¨ç”¨æˆ·æœªæä¾›æ•°æ®æ—¶ï¼Œæ‰ä½¿ç”¨ã€è”ç½‘æ£€ç´¢äº‹å®ã€‘æˆ–é€šç”¨å­¦æœ¯çŸ¥è¯†ã€‚
3.  **ä¸¥ç¦å¤¸å¤§**: 
    -   **ç¦æ­¢**: â€œå¡«è¡¥ç©ºç™½â€ã€â€œå›½å†…é¦–åˆ›â€ã€â€œå®Œç¾è§£å†³â€ã€‚
    -   **å¿…é¡»ç”¨**: â€œä¸°å¯Œäº†...è§†è§’â€ã€â€œæä¾›äº†å®è¯å‚è€ƒâ€ã€â€œä¼˜åŒ–äº†...â€ã€‚
4.  **ä¸¥ç¦æé€ **: æ— è®ºæ˜¯ç”¨æˆ·æ•°æ®è¿˜æ˜¯æ£€ç´¢æ•°æ®ï¼Œéƒ½å¿…é¡»ä¿æŒé€»è¾‘è‡ªæ´½ï¼Œä¸¥ç¦å‡­ç©ºæœæ’°å®éªŒç»“æœã€‚
5.  **æ–‡ä»¶å¼•ç”¨**: **ä¸¥ç¦ç¼–é€ ã€Šã€‹å†…çš„æ”¿ç­–/æ–‡ä»¶/è‘—ä½œåç§°**ã€‚å¿…é¡»ç¡®ä¿è¯¥æ”¿ç­–/æ–‡ä»¶/è‘—ä½œï¼Œåœ¨çœŸå®ä¸–ç•Œå­˜åœ¨ä¸”åç§°å®Œå…¨å‡†ç¡®ã€‚å¦‚æœä¸ç¡®å®šçœŸå®å…¨ç§°ï¼Œ**ä¸¥ç¦ä½¿ç”¨ä¹¦åå·**ï¼Œä»…æè¿°å…¶å†…å®¹å³å¯ã€‚


### **ç­–ç•¥C: ç« èŠ‚ä¸“å±é€»è¾‘**
{section_rule}

{ref_instruction}

{visuals_instruction}

### **ç­–ç•¥E: å­—æ•°æ§åˆ¶**
{word_count_strategy}
**æ‰©å†™æŠ€å·§**: å¦‚æœå­—æ•°ä¸è¶³ï¼Œè¯·å¯¹æ ¸å¿ƒæ¦‚å¿µè¿›è¡Œå®šä¹‰æ‰©å±•ï¼Œæˆ–å¢åŠ â€œä¸¾ä¾‹è¯´æ˜â€ã€â€œå¯¹æ¯”åˆ†æâ€ã€â€œç†è®ºæ”¯æ’‘â€ç­‰ç¯èŠ‚ï¼Œ**ä¸¥ç¦**é€šè¿‡é‡å¤åºŸè¯å‡‘å­—æ•°ã€‚

### **ç­–ç•¥G: ç»“æ„ä¸è¾¹ç•Œæ§åˆ¶ (CRITICAL - ç»å¯¹ç¦æ­¢é¡¹)**
1.  **ç¦æ­¢è‡ªæ‹Ÿæ ‡é¢˜**: è¾“å‡ºå†…å®¹**ä¸¥ç¦åŒ…å«**ä»»ä½• Markdown æ ‡é¢˜ç¬¦å·ï¼ˆ#ã€##ã€###ï¼‰ã€‚
    -   é”™è¯¯ç¤ºä¾‹ï¼š`### 1.1 èƒŒæ™¯åˆ†æ`
    -   æ­£ç¡®æ“ä½œï¼šç›´æ¥å¼€å§‹å†™èƒŒæ™¯åˆ†æçš„**æ­£æ–‡æ®µè½**ã€‚
2.  **ç¦æ­¢è¶Šç•Œ**: **ä¸¥ç¦**æ’°å†™ä¸‹ä¸€ä¸ªç« èŠ‚çš„å†…å®¹ã€‚åªå…³æ³¨å½“å‰ç« èŠ‚ï¼š**â€œ{current_chapter_title}â€**ã€‚
3.  **ç¦æ­¢åˆ†ç‚¹**: é™¤éæ˜¯â€œç ”ç©¶æ–¹æ³•â€ç« èŠ‚ï¼Œå¦åˆ™ä¸¥ç¦ä½¿ç”¨ `1.` `2.` æˆ– `*` è¿›è¡Œç½—åˆ—ï¼Œè¯·ç”¨â€œé¦–å…ˆã€å…¶æ¬¡ã€æ­¤å¤–â€ç­‰è¿æ¥è¯å†™æˆè¿è´¯æ®µè½ã€‚

è¯·å¼€å§‹å†™ä½œã€‚
"""



class PaperAutoWriter:
    def __init__(self, api_key: str, base_url: str, model: str):
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model

    def _call_llm(self, system_prompt: str, user_prompt: str) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
                temperature=0.7, stream=False
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"[Error: {str(e)}]"

    def _research_phase(self, topic: str) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä¸¥è°¨æ•°æ®åˆ†æå¸ˆã€‚åˆ—å‡ºå…³äºä¸»é¢˜çš„çœŸå®æ•°æ®ã€æ”¿ç­–ã€‚ä½¿ç”¨åŠè§’æ•°å­—ã€‚"},
                    {"role": "user", "content": f"æ£€ç´¢å…³äº'{topic}'çš„çœŸå®äº‹å®ï¼š"}
                ],
                temperature=0.3, stream=False
            )
            return response.choices[0].message.content.strip()
        except: 
            return ""

    def _extract_chapter_num(self, title: str) -> str:
        match_digit = re.match(r'^(\d+)', title.strip())
        if match_digit: return match_digit.group(1)
        match_cn = re.match(r'^ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ç« |éƒ¨åˆ†]', title.strip())
        if match_cn:
            cn_map = {'ä¸€':'1','äºŒ':'2','ä¸‰':'3','å››':'4','äº”':'5','å…­':'6','ä¸ƒ':'7','å…«':'8','ä¹':'9','å':'10'}
            return cn_map.get(match_cn.group(1), "")
        return ""

    def generate_stream(self, task_id: str, title: str, chapters: List[Dict], references_raw: str, custom_data: str, check_status_func, initial_context: str = "") -> Generator[str, None, None]:
        ref_manager = ReferenceManager(references_raw)
        yield f"data: {json.dumps({'type': 'log', 'msg': 'åˆå§‹åŒ–...'})}\n\n"
        chapter_ref_map = ref_manager.distribute_references_smart(chapters)
        
        full_content = f"# {title}\n\n"
        context = initial_context if initial_context else "è®ºæ–‡å¼€å¤´"
        
        for i, chapter in enumerate(chapters):
            while check_status_func() == "paused":
                time.sleep(1)
            if check_status_func() == "stopped": 
                yield f"data: {json.dumps({'type': 'log', 'msg': 'âš ï¸ æ”¶åˆ°åœæ­¢æŒ‡ä»¤ï¼Œæ­£åœ¨ä¸­æ–­...'})}\n\n"
                break
            
            sec_title = chapter['title']
            if chapter.get('is_parent', False):
                full_content += f"## {sec_title}\n\n"
                md_content = f"## {sec_title}\n\n"
                yield f"data: {json.dumps({'type': 'content', 'md': md_content})}\n\n"
                continue

            target = int(chapter.get('words', 500))
            assigned_refs = chapter_ref_map.get(i, [])
            ref_manager.set_current_chapter_refs(assigned_refs)
            chapter_num = self._extract_chapter_num(sec_title)
            
            yield f"data: {json.dumps({'type': 'log', 'msg': f'æ­£åœ¨æ’°å†™: {sec_title}'})}\n\n"
            
            facts_context = ""
            if "æ‘˜è¦" not in sec_title and "ç»“è®º" not in sec_title:
                if custom_data and len(custom_data.strip()) > 5:
                    yield f"data: {json.dumps({'type': 'log', 'msg': f'   - æ­£åœ¨æŒ‚è½½ç”¨æˆ·æä¾›çš„çœŸå®æ•°æ®...'})}\n\n"
                    cleaned_data = TextCleaner.convert_cn_numbers(custom_data)
                    facts_context = f"\nã€ç”¨æˆ·æä¾›çš„çœŸå®æ•°æ® (æœ€é«˜ä¼˜å…ˆçº§)ã€‘:\n{cleaned_data}\n\nè¯·ä¸¥æ ¼åŸºäºä»¥ä¸Šæ•°æ®è¿›è¡Œè®ºè¿°å’Œåˆ†æã€‚"
                else:
                    yield f"data: {json.dumps({'type': 'log', 'msg': f'   - æœªæ£€æµ‹åˆ°ç”¨æˆ·æ•°æ®ï¼Œæ­£åœ¨æ£€ç´¢ç½‘ç»œ/çŸ¥è¯†åº“æ•°æ®...'})}\n\n"
                    facts = self._research_phase(f"{title} - {sec_title}")
                    if facts:
                        facts = TextCleaner.convert_cn_numbers(facts)
                        facts_context = f"\nã€è”ç½‘æ£€ç´¢äº‹å®åº“ã€‘:\n{facts}"

            # 1. æ„å»º Prompt
            if "æ‘˜è¦" in sec_title or "Abstract" in sec_title:
                sys_prompt = get_academic_thesis_prompt(target, [r[1] for r in assigned_refs], sec_title, chapter_num)
                user_prompt = f"é¢˜ç›®ï¼š{title}\nç« èŠ‚ï¼š{sec_title}\nè¦æ±‚ï¼šè¯·ç›´æ¥è¾“å‡ºæ‘˜è¦çš„æ­£æ–‡å†…å®¹ï¼Œä¸¥ç¦è¾“å‡ºâ€œ### æ‘˜è¦â€æˆ–â€œ### Abstractâ€ç­‰æ ‡é¢˜ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§â€œæ‘˜è¦æ­£æ–‡ + å…³é”®è¯â€çš„æ ¼å¼è¾“å‡ºã€‚"
            else:
                sys_prompt = get_academic_thesis_prompt(target, [r[1] for r in assigned_refs], sec_title, chapter_num)
                user_prompt = f"é¢˜ç›®ï¼š{title}\nç« èŠ‚ï¼š{sec_title}\nå‰æ–‡ï¼š{context[-600:]}\nå­—æ•°ï¼š{target}\n{facts_context}"
            
            # 2. è°ƒç”¨ LLM
            raw_content = self._call_llm(sys_prompt, user_prompt)

            # 3. æ™ºèƒ½å­—æ•°æ‰©å†™/ç²¾ç®€ (ä»…å¯¹éæ‘˜è¦ç« èŠ‚)
            current_len = len(re.sub(r'\s', '', raw_content))
            if "æ‘˜è¦" not in sec_title and "Abstract" not in sec_title:
                if current_len < target * 0.6: 
                    yield f"data: {json.dumps({'type': 'log', 'msg': f'   - å­—æ•°ä¼˜åŒ–: æ­£åœ¨æ‰©å……å†…å®¹ ({current_len}/{target})...'})}\n\n"
                    expand_prompt = (
                        f"å½“å‰å­—æ•°({current_len})ä¸ç›®æ ‡({target})å·®è·è¾ƒå¤§ã€‚\n"
                        f"è¯·**æ‰©å†™**ä¸Šè¿°å†…å®¹ã€‚çº¢çº¿è¦æ±‚ï¼š\n"
                        f"1. **ä¸¥ç¦**åˆ é™¤åŸæ–‡ä¸­çš„ä»»ä½• `[REF]` å¼•ç”¨æ ‡è®°ã€‚\n"
                        f"2. å¢åŠ å…·ä½“æ¡ˆä¾‹ã€ç†è®ºåˆ†ææˆ–æ•°æ®å¯¹æ¯”ã€‚\n"
                        f"3. **ä¸¥ç¦**è¾“å‡ºâ€œå¥½çš„â€ã€â€œæ‰©å†™å¦‚ä¸‹â€ç­‰åºŸè¯ï¼Œç›´æ¥è¾“å‡ºæ‰©å†™åçš„æ­£æ–‡ã€‚\n"
                    )
                    try:
                        resp = self.client.chat.completions.create(
                            model=self.model,
                            messages=[
                                {"role": "system", "content": sys_prompt},
                                {"role": "user", "content": user_prompt},
                                {"role": "assistant", "content": raw_content},
                                {"role": "user", "content": expand_prompt}
                            ],
                            temperature=0.7
                        )
                        raw_content = resp.choices[0].message.content.strip() # æ›´æ–° raw_content
                    except Exception as e:
                        print(f"æ‰©å†™å¤±è´¥: {e}")

                elif current_len > target * 1.4:
                    yield f"data: {json.dumps({'type': 'log', 'msg': f'   - å­—æ•°ä¼˜åŒ–: æ­£åœ¨ç²¾ç®€å†…å®¹ ({current_len}/{target})...'})}\n\n"
                    condense_prompt = (
                        f"å½“å‰å­—æ•°({current_len})è¿œè¶…ç›®æ ‡({target})ã€‚\n"
                        f"è¯·**ç²¾ç®€**ä¸Šè¿°å†…å®¹ã€‚çº¢çº¿è¦æ±‚ï¼š\n"
                        f"1. **å¿…é¡»ä¿ç•™æ‰€æœ‰ `[REF]` å¼•ç”¨æ ‡è®°**ï¼Œç»å¯¹ä¸èƒ½åˆ å‡å‚è€ƒæ–‡çŒ®ã€‚\n"
                        f"2. åˆ é™¤é‡å¤çš„å½¢å®¹è¯ï¼Œä¿ç•™æ ¸å¿ƒè®ºç‚¹ã€‚\n"
                        f"3. **ä¸¥ç¦**è¾“å‡ºâ€œå¥½çš„â€ç­‰åºŸè¯ï¼Œç›´æ¥è¾“å‡ºç»“æœã€‚\n"
                    )
                    try:
                        resp = self.client.chat.completions.create(
                            model=self.model,
                            messages=[
                                {"role": "system", "content": sys_prompt},
                                {"role": "user", "content": user_prompt},
                                {"role": "assistant", "content": raw_content},
                                {"role": "user", "content": condense_prompt}
                            ],
                            temperature=0.7
                        )
                        raw_content = resp.choices[0].message.content.strip() # æ›´æ–° raw_content
                    except Exception as e:
                        print(f"ç²¾ç®€å¤±è´¥: {e}")

            # 4. [ä¿®å¤ 3 ç”Ÿæ•ˆ] å¼ºåŠ›æ¸…æ´—æ‘˜è¦æ ‡é¢˜ (æ›´æ–° raw_content)
            if "æ‘˜è¦" in sec_title or "Abstract" in sec_title:
                clean_content = raw_content.strip()
                clean_content = re.sub(r'^#+\s*(æ‘˜è¦|Abstract)\s*', '', clean_content, flags=re.IGNORECASE).strip()
                if clean_content.startswith("æ‘˜è¦") and len(clean_content) < 10:
                    clean_content = clean_content[2:].strip()
                if clean_content.startswith("Abstract") and len(clean_content) < 15:
                    clean_content = clean_content[8:].strip()
                raw_content = clean_content # ã€å…³é”®ä¿®æ”¹ã€‘å°†æ¸…æ´—ç»“æœèµ‹å€¼å› raw_content

            # 5. é€šç”¨æ ‡é¢˜æ¸…æ´— (ç§»é™¤æ­£æ–‡ç¬¬ä¸€è¡Œé‡å¤çš„æ ‡é¢˜)
            temp_lines = raw_content.strip().split('\n')
            if temp_lines:
                first_line_core = re.sub(r'[#*\s]', '', temp_lines[0])
                title_core = re.sub(r'[#*\s]', '', sec_title)
                if title_core in first_line_core and len(first_line_core) < len(title_core) + 8:
                    raw_content = '\n'.join(temp_lines[1:]) # æ›´æ–° raw_content

            # 6. å¼•ç”¨å¤„ç†ä¸æ ¼å¼åŒ–
            # æ³¨æ„ï¼šå¿…é¡»ç¡®ä¿ reference.py ä¸­çš„ process_text_deterministic ä¹Ÿå·²æ›´æ–°
            processed_content = ref_manager.process_text_deterministic(raw_content)
            processed_content = TextCleaner.convert_cn_numbers(processed_content)
            
            lines = processed_content.split('\n')
            formatted_lines = []
            for line in lines:
                line = line.strip()
                line = re.sub(r'^[\(ï¼ˆ]ç©ºä¸¤æ ¼[\)ï¼‰]', '', line) 
                line = re.sub(r'^ç©ºæ ¼', '', line)
                # æ’é™¤å·²ç»æ˜¯ç¼©è¿›çš„ã€æ ‡é¢˜ã€è¡¨æ ¼ã€ä»£ç å—ç­‰
                if (line and not line.startswith('ã€€ã€€') and not line.startswith('#') and 
                    not line.startswith('|') and not line.startswith('```') and "import" not in line and "plt." not in line):
                    line = 'ã€€ã€€' + line 
                formatted_lines.append(line)
            final_content = '\n\n'.join(formatted_lines)

            section_md = f"## {sec_title}\n\n{final_content}\n\n"
            full_content += section_md
            context = final_content
            yield f"data: {json.dumps({'type': 'content', 'md': section_md})}\n\n"

        if check_status_func() != "stopped":
            bib = ref_manager.generate_bibliography()
            full_content += bib
            yield f"data: {json.dumps({'type': 'content', 'md': bib})}\n\n"
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
        else:
            yield f"data: {json.dumps({'type': 'log', 'msg': 'ğŸ›‘ ä»»åŠ¡å·²å®Œå…¨ç»ˆæ­¢ (å·²è·³è¿‡åç»­å†…å®¹)'})}\n\n"