
import re
import json
import time
from openai import OpenAI
from typing import Dict, List, Generator
from .reference import ReferenceManager
from .word import TextCleaner
from .prompts import get_rewrite_prompt, get_word_distribution_prompt, get_academic_thesis_prompt
import concurrent.futures

class PaperAutoWriter:
    def __init__(self, api_key: str, base_url: str, model: str):
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        # ä¸»çº¿ç¨‹å®¢æˆ·ç«¯
        self.main_client = OpenAI(api_key=api_key, base_url=base_url, timeout=120.0)

    # --------------------------------------------------------------------------
    # è¾…åŠ©æ–¹æ³•ï¼šClient éš”ç¦»è°ƒç”¨
    # --------------------------------------------------------------------------
    
    def _call_llm_with_client(self, client, system_prompt: str, user_prompt: str) -> str:
        """[åŸºç¡€æ–¹æ³•] ä½¿ç”¨æŒ‡å®šçš„ client å®ä¾‹è°ƒç”¨ LLM"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
                    temperature=0.7, 
                    stream=False
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                print(f"âš ï¸ [LLM Error] Attempt {attempt+1}/{max_retries}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    raise e # æŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚æ•è·

    def _call_llm(self, system_prompt: str, user_prompt: str) -> str:
        """[ä¸»çº¿ç¨‹] è°ƒç”¨æ–¹æ³•"""
        return self._call_llm_with_client(self.main_client, system_prompt, user_prompt)

    # --------------------------------------------------------------------------
    # è”ç½‘æœç´¢æ–¹æ³•
    # --------------------------------------------------------------------------
    
    def _research_phase_with_client(self, client, topic: str) -> str:
        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä¸¥è°¨æ•°æ®åˆ†æå¸ˆã€‚åˆ—å‡ºå…³äºä¸»é¢˜çš„çœŸå®æ•°æ®ã€æ”¿ç­–ã€‚"},
                    {"role": "user", "content": f"æ£€ç´¢å…³äº'{topic}'çš„çœŸå®äº‹å®ï¼š"}
                ],
                temperature=0.3, stream=False
            )
            return response.choices[0].message.content.strip()
        except: 
            return ""

    def _research_phase(self, topic: str) -> str:
        return self._research_phase_with_client(self.main_client, topic)

    # --------------------------------------------------------------------------
    # çŠ¶æ€æ£€æŸ¥
    # --------------------------------------------------------------------------

    def _check_process_status(self, check_status_func) -> bool:
        while check_status_func() == "paused":
            time.sleep(1)
        return check_status_func() == "stopped"

    def _extract_chapter_num(self, title: str) -> str:
        match = re.match(r'^(\d+)', title.strip())
        return match.group(1) if match else ""

    def _determine_header_prefix(self, chapter: Dict, sec_title: str) -> str:
        level = 2
        if 'level' in chapter: level = int(chapter['level']) + 1
        return "#" * min(max(level, 2), 6)

    def _clean_and_format(self, raw_content: str, sec_title: str, ref_manager) -> str:
        if "æ‘˜è¦" in sec_title or "Abstract" in sec_title:
            raw_content = re.sub(r'^#+\s*(æ‘˜è¦|Abstract)\s*', '', raw_content, flags=re.IGNORECASE).strip()
        
        dirty_patterns = [r'[\(ï¼ˆ]æ¥ä¸Šæ–‡[\)ï¼‰]', r'[\(ï¼ˆ]ç©ºä¸¤æ ¼[\)ï¼‰]', r'^\.\.\.', r'æ¥ä¸Šæ–‡ï¼š']
        for p in dirty_patterns:
            raw_content = re.sub(p, '', raw_content)

        if ref_manager:
            raw_content = ref_manager.process_text_deterministic(raw_content)
        
        processed = TextCleaner.convert_cn_numbers(raw_content)
        lines = []
        for line in processed.split('\n'):
            line = line.strip()
            if (line and not line.startswith('ã€€ã€€') and not line.startswith('#') and 
                not line.startswith('|') and not line.startswith('```') and "import" not in line):
                line = 'ã€€ã€€' + line 
            lines.append(line)
        return '\n\n'.join(lines)

    def _refine_content(self, raw_content: str, target: int, sec_title: str, sys_prompt: str, user_prompt: str) -> Generator[str, None, str]:
        content_no_code = re.sub(r'```[\s\S]*?```', '', raw_content)
        current_len = len(re.sub(r'\s', '', content_no_code))
        if target < 300: return raw_content
        
        # ç®€åŒ–ç‰ˆç²¾ç®€é€»è¾‘ï¼Œé˜²æ­¢é€’å½’æŠ¥é”™
        return raw_content

    # --------------------------------------------------------------------------
    # [æ ¸å¿ƒä¿®å¤] å•ç« èŠ‚å¤„ç†å‡½æ•° (å¢åŠ è¯¦ç»†Debug Log)
    # --------------------------------------------------------------------------
    
    def _process_single_chapter(self, task_bundle):
        """çº¿ç¨‹å·¥ä½œå‡½æ•°"""
        i = -1
        sec_title = "æœªçŸ¥ç« èŠ‚"
        logs = []
        
        try:
            if len(task_bundle) < 12: 
                return { "index": -1, "type": "error", "msg": f"å‚æ•°ä¸è¶³: {len(task_bundle)}", "logs": [] }

            (api_key, base_url, model, task_id, title, chapter, 
             ref_domestic, ref_foreign,  # <--- è¿™é‡Œæ¥æ”¶åˆ†å¼€çš„æ–‡çŒ®
             custom_data, context_summary, index_val, 
             full_outline_str) = task_bundle
            
            i = index_val
            sec_title = chapter.get('title', 'æ— æ ‡é¢˜')
            target = int(chapter.get('words', 500))
            is_parent = chapter.get('is_parent', False)

            # Debug Print
            # print(f"[Thread {i}] å¤„ç†ç« èŠ‚: {sec_title} | å­—æ•°: {target}")

            # 2. æ ‡é¢˜å¤„ç† (çˆ¶èŠ‚ç‚¹ç›´æ¥è¿”å›)
            header_prefix = self._determine_header_prefix(chapter, sec_title)
            if is_parent or target <= 0:
                return {
                    "index": i, "type": "header_only", 
                    "content": f"{header_prefix} {sec_title}\n\n",
                    "logs": [f"ç”Ÿæˆæ ‡é¢˜: {sec_title}"]
                }

            # 3. åˆå§‹åŒ– Client
            local_client = OpenAI(api_key=api_key, base_url=base_url, timeout=120.0)
            
            chapter_num = self._extract_chapter_num(sec_title)
            logs.append(f"ğŸš€ [å¹¶å‘å¯åŠ¨] æ­£åœ¨æ’°å†™: {sec_title}")

            # 4. æ•°æ®ä¸Šä¸‹æ–‡å‡†å¤‡
            facts_context = ""
            use_data_flag = chapter.get('use_data', False)
            has_user_data = False
            
            if "æ‘˜è¦" not in sec_title and use_data_flag:
                if custom_data and len(custom_data.strip()) > 5:
                    cleaned_data = TextCleaner.convert_cn_numbers(custom_data)
                    facts_context += f"\nã€ç”¨æˆ·çœŸå®æ•°æ®ã€‘:\n{cleaned_data}\n"
                    has_user_data = True
                
                # logs.append(f"   - ğŸ” [å¹¶è¡Œæ£€ç´¢] è¡¥å……æ•°æ®...")
                facts = self._research_phase_with_client(local_client, f"{title} - {sec_title} æ•°æ®")
                if facts:
                    facts_context += f"\nã€è”ç½‘è¡¥å……æ•°æ®ã€‘:\n{facts}\n"

            # [ä¿®æ”¹] 5. æ™ºèƒ½æ–‡çŒ®é€‰æ‹©é€»è¾‘
            target_ref_list = []
            
            # åˆ¤æ–­é€»è¾‘ï¼šæ ¹æ®æ ‡é¢˜å…³é”®è¯é”å®šæ–‡çŒ®åº“
            is_domestic_review = "å›½å†…" in sec_title and ("ç°çŠ¶" in sec_title or "ç»¼è¿°" in sec_title)
            is_foreign_review = "å›½å¤–" in sec_title and ("ç°çŠ¶" in sec_title or "ç»¼è¿°" in sec_title)
            
            raw_ref_text = ""

            if is_domestic_review:
                logs.append(f"   - ğŸ“š é”å®šï¼šå›½å†…å‚è€ƒæ–‡çŒ®")
                raw_ref_text = ref_domestic
            elif is_foreign_review:
                logs.append(f"   - ğŸ“š é”å®šï¼šå›½å¤–å‚è€ƒæ–‡çŒ®")
                raw_ref_text = ref_foreign
            else:
                # å…¶ä»–ç« èŠ‚ï¼ˆå¦‚ç†è®ºã€æ­£æ–‡ï¼‰ï¼Œä¸ºäº†å¼•ç”¨ä¸°å¯Œåº¦ï¼Œåˆå¹¶ä¸¤è€…
                # ä¸­é—´åŠ æ¢è¡Œç¬¦é˜²æ­¢ç²˜è¿
                raw_ref_text = f"{ref_domestic}\n{ref_foreign}"

            # è§£æä¸ºåˆ—è¡¨ (å–å‰8æ¡ï¼Œé˜²æ­¢ Token çˆ†ç‚¸)
            if raw_ref_text:
                target_ref_list = [line.strip() for line in raw_ref_text.split('\n') if line.strip()][:8]

            # 6. Prompt æ„å»º
            sys_prompt = get_academic_thesis_prompt(
                target, 
                target_ref_list, # ä¼ å…¥ç­›é€‰åçš„åˆ—è¡¨
                sec_title, 
                chapter_num, 
                has_user_data, 
                full_outline=full_outline_str
            )
            user_prompt = f"é¢˜ç›®ï¼š{title}\nç« èŠ‚ï¼š{sec_title}\nå‰æ–‡æ‘˜è¦ï¼š{context_summary}\nã€é‡è¦çº¦æŸã€‘ç›®æ ‡å­—æ•°ï¼š{target}å­—\n{facts_context}"

            # 7. LLM è°ƒç”¨
            raw_content = self._call_llm_with_client(local_client, sys_prompt, user_prompt)

            # 8. ç®€å•å­—æ•°æ£€æŸ¥ä¸æ‰©å†™ (çœç•¥è¯¦ç»†é€»è¾‘ï¼Œä¿æŒåŸæœ‰å³å¯)
            content_no_code = re.sub(r'```[\s\S]*?```', '', raw_content)
            current_len = len(re.sub(r'\s', '', content_no_code))
            if "æ‘˜è¦" not in sec_title and target > 300 and current_len < target * 0.5:
                 try:
                    raw_content = self._call_llm_with_client(local_client, sys_prompt, user_prompt + "\n\nè¯·å¤§å¹…æ‰©å†™ï¼Œå¢åŠ ç»†èŠ‚ã€‚")
                 except: pass

            # 9. æ¸…æ´—
            # è¿™é‡Œçš„ ref_manager ä¼  None å³å¯ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ Prompt é‡Œå·²ç»å¤„ç†äº†å¼•ç”¨æ ¼å¼
            final_content = self._clean_and_format(raw_content, sec_title, None)
            section_md = f"{header_prefix} {sec_title}\n\n{final_content}\n\n"
            
            return {
                "index": i, "type": "content", 
                "content": section_md, "raw_text": final_content, "logs": logs
            }

        except Exception as e:
            err_msg = f"âŒ {sec_title} å¼‚å¸¸: {str(e)}"
            print(f"[Thread {i}] ERROR: {err_msg}")
            return { "index": i, "type": "error", "msg": str(e), "logs": [err_msg] }

    # --------------------------------------------------------------------------
    # å¹¶å‘ç”Ÿæˆå™¨
    # --------------------------------------------------------------------------
    def _format_outline(self, chapters: List[Dict]) -> str:
        outline_lines = []
        for ch in chapters:
            title = ch.get('title', 'æœªå‘½å')
            outline_lines.append(f"- {title}")
        return "\n".join(outline_lines)

    def generate_stream(self, task_id: str, title: str, chapters: List[Dict], ref_domestic: str, ref_foreign: str, custom_data: str, check_status_func, initial_context: str = "") -> Generator[str, None, None]:
        
        # è¿™é‡Œçš„ ref_manager ä¸»è¦ç”¨äºæœ€åç”Ÿæˆæ–‡æœ«çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼Œæ‰€ä»¥åˆå¹¶ä¸¤è€…
        combined_refs = f"{ref_domestic}\n{ref_foreign}"
        ref_manager = ReferenceManager(combined_refs)
        
        yield f"data: {json.dumps({'type': 'log', 'msg': 'ğŸš€ å¯åŠ¨é«˜å¹¶å‘ç”Ÿæˆå¼•æ“ (Max Threads=8)...'})}\n\n"
        
        full_content = f"# {title}\n\n"
        global_context = initial_context if initial_context else f"è®ºæ–‡é¢˜ç›®ï¼šã€Š{title}ã€‹"
        
        # é¢„å…ˆç”Ÿæˆå…¨æ–‡å¤§çº²æ–‡æœ¬å­—ç¬¦ä¸²
        full_outline_str = self._format_outline(chapters)

        MAX_WORKERS = 8
        all_futures = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # 1. æäº¤ä»»åŠ¡
            for i, chapter in enumerate(chapters):
                if self._check_process_status(check_status_func): break
                
                task_bundle = (
                    self.api_key, self.base_url, self.model,
                    task_id, title, chapter, 
                    ref_domestic, ref_foreign,  # <--- æ–°å¢çš„ä¸¤ä¸ªå‚æ•°
                    custom_data, global_context[:800], i,
                    full_outline_str
                )
                future = executor.submit(self._process_single_chapter, task_bundle)
                all_futures.append(future)
            
            # 2. è·å–ç»“æœ
            for future in all_futures:
                if self._check_process_status(check_status_func):
                    executor.shutdown(wait=False)
                    break
                
                while True:
                    try:
                        result = future.result(timeout=1)
                        for log in result.get('logs', []):
                            yield f"data: {json.dumps({'type': 'log', 'msg': log})}\n\n"
                        
                        if result['type'] == 'error':
                            yield f"data: {json.dumps({'type': 'log', 'msg': result['msg']})}\n\n"
                            break

                        if result['type'] in ['content', 'header_only']:
                            content_md = result['content']
                            full_content += content_md
                            yield f"data: {json.dumps({'type': 'content', 'md': content_md})}\n\n"
                            global_context += result.get('raw_text', '')[-200:]
                        
                        break
                    except concurrent.futures.TimeoutError:
                        yield f": keep-alive\n\n"
                        if self._check_process_status(check_status_func): return
                    except Exception as e:
                        yield f"data: {json.dumps({'type': 'log', 'msg': f'âŒ ä¸»çº¿ç¨‹å¼‚å¸¸: {str(e)}'})}\n\n"
                        break

        if check_status_func() != "stopped":
            # ç”Ÿæˆæ–‡æœ«å‚è€ƒæ–‡çŒ®åˆ—è¡¨
            bib = ref_manager.generate_bibliography()
            full_content += bib
            yield f"data: {json.dumps({'type': 'content', 'md': bib})}\n\n"
            yield f"data: {json.dumps({'type': 'done'})}\n\n"

    # --------------------------------------------------------------------------
    # å…¶ä»–å…¬å…±æ–¹æ³•
    # --------------------------------------------------------------------------

    def rewrite_chapter(self, title: str, section_title: str, user_instruction: str, context: str, custom_data: str, original_content: str = "") -> str:
        sys_prompt = get_rewrite_prompt(title, section_title, user_instruction, context[-800:], custom_data, original_content)
        user_prompt = f"è®ºæ–‡é¢˜ç›®ï¼š{title}\nè¯·ä¿®æ”¹ç« èŠ‚ï¼š{section_title}\nç”¨æˆ·çš„å…·ä½“ä¿®æ”¹æ„è§ï¼š{user_instruction}"
        return self._call_llm(sys_prompt, user_prompt)
    
    def plan_word_count(self, total_words: int, outline_list: List[str]) -> Dict[str, Dict]:
        outline_str = "\n".join(outline_list)
        prompt = get_word_distribution_prompt(total_words, outline_str)
        
        try:
            response = self.main_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼è¾“å‡º JSON çš„å­¦æœ¯è§„åˆ’å¸ˆã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                stream=False,
                response_format={"type": "json_object"}
            )
            content = response.choices[0].message.content.strip()
            if content.startswith("```"): content = re.sub(r'```json|```', '', content).strip()
            
            try:
                raw_map = json.loads(content)
            except json.JSONDecodeError:
                return {}

            standardized_map = {}
            for k, v in raw_map.items():
                if "total" in k.lower(): continue
                if isinstance(v, dict):
                    w = int(v.get('words', 0))
                    d = v.get('needs_data', False)
                    if isinstance(d, str): d = d.lower() == 'true'
                    standardized_map[k] = {"words": w, "needs_data": d}
                elif isinstance(v, (int, float)):
                    standardized_map[k] = {"words": int(v), "needs_data": False}
            
            current_total = sum(item['words'] for item in standardized_map.values())
            if current_total == 0: return standardized_map

            ratio = total_words / current_total
            final_map = {}
            for k, v in standardized_map.items():
                final_map[k] = {"words": int(v['words'] * ratio), "needs_data": v['needs_data']}
            
            # è¯¯å·®ä¿®æ­£
            new_total = sum(item['words'] for item in final_map.values())
            diff = total_words - new_total
            if diff != 0 and final_map:
                max_key = max(final_map, key=lambda k: final_map[k]['words'])
                final_map[max_key]['words'] += diff
                
            return final_map
        except Exception as e:
            print(f"Plan error: {e}")
            return {}