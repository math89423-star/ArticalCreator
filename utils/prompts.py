import re
import json
import time
from typing import List, Dict, Generator
from openai import OpenAI
from typing import Dict, Generator
from .reference import ReferenceManager
from .word import TextCleaner


def get_academic_thesis_prompt(target_words: int, ref_content_list: List[str], current_chapter_title: str, chapter_num: str, has_user_data: bool = False) -> str:
    
    # ------------------------------------------------------------------
    # 1. ç« èŠ‚ä¸“å±é€»è¾‘
    # ------------------------------------------------------------------
    section_rule = ""
    is_cn_abstract = "æ‘˜è¦" in current_chapter_title
    is_en_abstract = "Abstract" in current_chapter_title and "æ‘˜è¦" not in current_chapter_title

    # åˆ¤æ–­æ˜¯å¦ä¸ºéœ€è¦å›¾è¡¨çš„ç« èŠ‚
    # [ä¿®æ”¹ç‚¹1] é€»è¾‘å‡çº§ï¼šå¦‚æœç« èŠ‚ååŒ…å«ç‰¹å®šè¯ OR æŒ‚è½½äº†ç”¨æˆ·æ•°æ®ï¼Œåˆ™å¿…é¡»å¼€å¯å›¾è¡¨
    needs_charts = False
    keywords = ["å®éªŒ", "æµ‹è¯•", "åˆ†æ", "ç»“æœ", "æ•°æ®", "è®¾è®¡", "å®ç°", "éªŒè¯", "Evaluation", "Analysis", "Design"]
    if (chapter_num and any(k in current_chapter_title for k in keywords)) or has_user_data:
        needs_charts = True
    
    # A. æ‘˜è¦
    if is_cn_abstract:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™æ‘˜è¦ä¸å…³é”®è¯**
**çº¢çº¿è§„åˆ™**: 
1. **ä¸¥ç¦è¾“å‡ºæ ‡é¢˜**: ä¸è¦è¾“å‡º "### æ‘˜è¦" æˆ– "### Abstract"ï¼Œç›´æ¥å¼€å§‹å†™æ­£æ–‡ã€‚
2. **ä¸­è‹±å¯¹ç…§**: å…ˆå†™ä¸­æ–‡éƒ¨åˆ†ï¼Œå†å†™è‹±æ–‡éƒ¨åˆ†ã€‚è‹¥æ²¡æœ‰è‹±æ–‡ï¼ˆAbstractï¼‰éƒ¨åˆ†åˆ™ä¸å†™è‹±æ–‡éƒ¨åˆ†ã€‚

**é€»è¾‘ç»“æ„**:
1. **ç ”ç©¶èƒŒæ™¯**: ç®€è¿°èƒŒæ™¯ï¼ˆçº¦50å­—ï¼‰ã€‚
2. **æ–¹æ³•åˆ›æ–°**: åšäº†ä»€ä¹ˆï¼Œç”¨äº†ä»€ä¹ˆæ–¹æ³•ï¼ˆçº¦100å­—ï¼‰ã€‚
3. **å…³é”®å‘ç°**: å¾—åˆ°äº†ä»€ä¹ˆæ•°æ®æˆ–ç»“è®ºï¼ˆçº¦100å­—ï¼‰ã€‚
4. **ç†è®ºè´¡çŒ®**: ä»·å€¼æ˜¯ä»€ä¹ˆï¼ˆçº¦50å­—ï¼‰ã€‚

**æ ¼å¼æ¨¡æ¿**:
   - ç›´æ¥è¾“å‡ºæ‘˜è¦æ­£æ–‡ã€‚
   - æœ€åä¸€è¡Œè¾“å‡ºï¼š**å…³é”®è¯**ï¼šè¯1ï¼›è¯2ï¼›è¯3
"""

    elif is_en_abstract:
        section_rule = """
**Current Task: Write English Abstract & Keywords**
**Requirements**:
1. **Language**: MUST be written in **English** only. No Chinese allowed.
2. **Content**: Translate the logic of a standard academic abstract (Background -> Method -> Results -> Conclusion).
3. **Format**: 
   - Output the abstract body directly.
   - Last line: **Keywords**: Word1; Word2; Word3
"""

    # B. èƒŒæ™¯ä¸æ„ä¹‰
    elif "èƒŒæ™¯" in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶èƒŒæ™¯**
**è¦æ±‚**:
1. **çœŸå®æ”¿ç­–**: å¿…é¡»ç»“åˆè¿‘å‡ å¹´ä¸­å›½çœŸå®å­˜åœ¨çš„å›½å®¶æ”¿ç­–ã€æœ€æ–°æ–‡ä»¶ã€é‡å¤§ç›¸å…³äº‹é¡¹ã€‚
2. **æ•°æ®æ”¯æ’‘**: éœ€è¦ä¸€ç‚¹çœŸå®æ•°æ®ä½œä¸ºèƒŒæ™¯æ”¯æ’‘ã€‚
3. **ç¯‡å¹…**: 350å­—å·¦å³ï¼Œä¸æ³›æ³›è€Œè°ˆã€‚
"""
    elif "æ„ä¹‰" in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶æ„ä¹‰**
**è¦æ±‚**:
1. **ç†è®ºæ„ä¹‰**: ä¸¥ç¦è¯´â€œå¡«è¡¥äº†ç©ºç™½â€ï¼Œå¿…é¡»è¯´â€œ**ä¸°å¯Œäº†...ç†è®ºæ¡†æ¶**â€æˆ–â€œ**ä¸º...æä¾›äº†å®è¯è¡¥å……**â€ã€‚
2. **å®é™…æ„ä¹‰**: è§£å†³å…·ä½“è¡Œä¸šæˆ–ç¤¾ä¼šç—›ç‚¹ã€‚
3. **ç¯‡å¹…**: 350å­—å·¦å³ã€‚
"""

    # C. å›½å†…å¤–ç ”ç©¶ç°çŠ¶
    elif any(k in current_chapter_title for k in ["ç°çŠ¶", "ç»¼è¿°", "Review"]):
        if ref_content_list:
            first_ref = ref_content_list[0]
            if len(ref_content_list) > 1:
                other_refs_prompt = "\n".join([f"{{æ–‡çŒ®{i+2}}}: {ref}" for i, ref in enumerate(ref_content_list[1:])])
            else:
                other_refs_prompt = "æ— åç»­æ–‡çŒ®"
            
            section_rule = f"""
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶ç°çŠ¶ (æ–‡çŒ®ç»¼è¿°)**
**æ ¸å¿ƒç›®æ ‡**ï¼šå°†æä¾›çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨è½¬åŒ–ä¸ºé€»è¾‘é€šé¡ºçš„å­¦æœ¯è¯„è¿°ã€‚

### **å¼•ç”¨è§„èŒƒ (é›¶å®¹å¿è§„åˆ™)**
1.  **ç¦æ­¢å‡ºç°ID**: æ­£æ–‡ä¸­**ç»å¯¹ç¦æ­¢**å‡ºç° "å‚è€ƒæ–‡çŒ®ID"ã€"æ–‡çŒ®1"ã€"Reference ID" ç­‰å­—æ ·ã€‚
2.  **å¼•ç”¨æ ¼å¼**: å¿…é¡»ä»æ–‡çŒ®å†…å®¹ä¸­æå–**ä½œè€…**å’Œ**å¹´ä»½**ï¼Œæ ¼å¼ä¸º `ä½œè€…(å¹´ä»½)`ã€‚
    -   *ä¾‹*: "Zhang (2023) æå‡ºäº†..." æˆ– "OpenAI (2024) å‘å¸ƒäº†..."
    -   *å¦‚æœæ‰¾ä¸åˆ°ä½œè€…*: ä½¿ç”¨ `ã€Šæ ‡é¢˜ã€‹(å¹´ä»½)`ã€‚
3.  **ç¦æ­¢æ¨¡ç³Š**: ä¸¥ç¦ä½¿ç”¨ "æŸå­¦è€…"ã€"æœ‰ç ”ç©¶"ã€"è¯¥ä½œå“" ç­‰æŒ‡ä»£ä¸æ˜çš„è¯ï¼Œ**å¿…é¡»æŒ‡åé“å§“**ã€‚
4.  **é¡ºåºä¸é¢‘æ¬¡**: 
    -   å¿…é¡»**ä¸¥æ ¼æŒ‰ç…§åˆ—è¡¨é¡ºåº**é€ä¸€è®ºè¿°ã€‚
    -   åˆ—è¡¨ä¸­çš„**æ¯ä¸€æ¡**æ–‡çŒ®éƒ½å¿…é¡»è¢«å¼•ç”¨**ä¸€æ¬¡ä¸”ä»…ä¸€æ¬¡**ã€‚
    -   æ¯æ®µè®ºè¿°ç»“æŸå¥æœ«å°¾å¿…é¡»åŠ  `[REF]` æ ‡è®°ã€‚

### **å†™ä½œé€»è¾‘**
1.  **ç¬¬ä¸€æ®µ (å¯¼è¯­)**: ç®€è¦æ¦‚æ‹¬è¯¥é¢†åŸŸçš„æ€»ä½“å‘å±•è¶‹åŠ¿ï¼ˆçº¦80å­—ï¼‰ã€‚
2.  **ç¬¬äºŒæ®µ (æ ¸å¿ƒç»¼è¿°)**: 
    -   **é¦–æ¡è¯¦è¿°**: é’ˆå¯¹ **{{æ–‡çŒ®1}}** ({first_ref}) è¿›è¡Œè¯¦ç»†è¯„è¿°ï¼ˆçº¦150å­—ï¼‰ã€‚å†™æ˜ï¼šä½œè€…+å¹´ä»½+æ ¸å¿ƒè´¡çŒ®+å±€é™æ€§ã€‚æ–‡æœ«åŠ  `[REF]`ã€‚
    -   **åç»­ä¸²è”**: ä¾æ¬¡å¯¹ **{{æ–‡çŒ®2}}** åŠåç»­æ–‡çŒ®è¿›è¡Œè¯„è¿°ã€‚
        -   *å¿…é¡»ä»æä¾›çš„æ–‡æœ¬ä¸­æå–çœŸå®ä½œè€…å’Œè§‚ç‚¹ï¼Œä¸¥ç¦ç¼–é€ ã€‚*
        -   ä½¿ç”¨è¿æ¥è¯ï¼ˆå¦‚"ä¸ä¹‹ç±»ä¼¼"ã€"ç„¶è€Œ"ã€"åœ¨æ­¤åŸºç¡€ä¸Š"ï¼‰å°†ä¸åŒæ–‡çŒ®é€»è¾‘ä¸²è”ã€‚
        -   æ ¼å¼ï¼š`ä½œè€…(å¹´ä»½) + è§‚ç‚¹/æ–¹æ³• + [REF]`ã€‚
3.  **ç¬¬ä¸‰æ®µ (è¯„è¿°)**: æ€»ç»“ä¸Šè¿°æ–‡çŒ®çš„å…±åŒä¸è¶³ï¼Œå¼•å‡ºæœ¬ç ”ç©¶çš„åˆ‡å…¥ç‚¹ã€‚

**å¾…ç»¼è¿°çš„æ–‡çŒ®åˆ—è¡¨ (è¯·ä»ä¸­æå–ä¿¡æ¯)**:
- {{æ–‡çŒ®1}}: {first_ref}
{other_refs_prompt}
"""
        else:
            section_rule = "**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶ç°çŠ¶**\nè¯·åŸºäºé€šç”¨å­¦æœ¯çŸ¥è¯†æ’°å†™ï¼Œä¿æŒæ€»åˆ†æ€»ç»“æ„ï¼Œå¼•ç”¨çœŸå®å­˜åœ¨çš„ç»å…¸æ–‡çŒ®ã€‚"

    # D. æ–‡çŒ®è¿°è¯„
    elif "è¿°è¯„" in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™æ–‡çŒ®è¿°è¯„**
**è¦æ±‚**: 
1. **ä¸å¼•ç”¨**: æ­¤éƒ¨åˆ†ä¸éœ€è¦å¼•ç”¨å…·ä½“æ–‡çŒ®ã€‚
2. **å†…å®¹**: æ€»ç»“å‰æ–‡æ–‡çŒ®çš„ä¸è¶³ï¼ŒæŒ‡å‡ºæœ¬ç ”ç©¶çš„åˆ‡å…¥ç‚¹ï¼ˆå€Ÿé‰´ä»€ä¹ˆï¼Œä¸°å¯Œä»€ä¹ˆï¼‰ã€‚
3. **ç¯‡å¹…**: ä¸€ä¸ªæ®µè½ï¼Œ300å­—å·¦å³ã€‚
"""

    # E. ç ”ç©¶å†…å®¹
    elif "ç ”ç©¶å†…å®¹" in current_chapter_title and "æ–¹æ³•" not in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶å†…å®¹**
**æ ¼å¼**: åˆ†æ®µå¼å›ç­”ã€‚
1. **å¯¼è¯­**: â€œæœ¬ç ”ç©¶ä¸»è¦ç ”ç©¶...ï¼Œå…·ä½“å†…å®¹å¦‚ä¸‹ï¼šâ€
2. **åˆ†ç« èŠ‚**: 
   - â€œç¬¬ä¸€éƒ¨åˆ†ï¼Œç»ªè®ºã€‚ä¸»è¦é˜è¿°...â€
   - â€œç¬¬äºŒéƒ¨åˆ†ï¼Œ...ã€‚åˆ†æäº†...â€
   - ...
**è¦æ±‚**: æ ¸å¿ƒç« èŠ‚è§£é‡Šçº¦200å­—ï¼Œè¯¦ç•¥å¾—å½“ã€‚
"""

    # F. ç ”ç©¶æ–¹æ³•
    elif "ç ”ç©¶æ–¹æ³•" in current_chapter_title:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™ç ”ç©¶æ–¹æ³•**
**æ ¼å¼**: åˆ†ç‚¹å›ç­”ï¼Œå¿…é¡»æ ‡åºå· (1. 2. 3.)ã€‚
**å¿…é€‰æ–¹æ³• (æŒ‰éœ€é€‰æ‹©)**:
1. **æ–‡çŒ®ç ”ç©¶æ³•**: (å¦‚æœ‰å‚è€ƒæ–‡çŒ®åˆ™å¿…é€‰)
2. **æ•°æ®åˆ†ææ³•**: (å¦‚æœ‰æ•°æ®åˆ†æåˆ™å¿…é€‰)
3. **å®è¯ç ”ç©¶æ³•/æ¡ˆä¾‹åˆ†ææ³•**: (æ ¹æ®é¢˜ç›®åˆ¤æ–­)
**è¦æ±‚**: ç»“åˆè®ºæ–‡ä¸»é¢˜è§£é‡Šä¸ºä»€ä¹ˆç”¨è¿™ä¸ªæ–¹æ³•ã€‚
"""

    # G. é€šç”¨æ­£æ–‡
    else:
        section_rule = """
**å½“å‰ä»»åŠ¡ï¼šæ’°å†™æ­£æ–‡åˆ†æ**
1. **é€»è¾‘ä¸»å¯¼**: æ ¸å¿ƒæ˜¯åˆ†ææ€è·¯ã€‚
2. **æ·±åº¦è®ºè¿°**: æ¯ä¸€æ®µéƒ½è¦æœ‰è§‚ç‚¹ã€æœ‰è®ºæ®ï¼ˆæ•°æ®æˆ–ç†è®ºï¼‰ã€æœ‰ç»“è®ºã€‚
"""

    # ------------------------------------------------------------------
    # 2. å¼•ç”¨æŒ‡ä»¤
    # ------------------------------------------------------------------
    ref_instruction = ""
    if ref_content_list and any(k in current_chapter_title for k in ["ç°çŠ¶", "ç»¼è¿°", "Review"]):
        ref_instruction = f"""
### **ç­–ç•¥D: å¼•ç”¨æ‰§è¡Œ (Token Strategy)**
æœ¬ç« èŠ‚å¿…é¡»å¼•ç”¨åˆ†é…çš„ {len(ref_content_list)} æ¡æ–‡çŒ®ã€‚
1.  **ä¸è¦ç”Ÿæˆåºå·**: ä¸è¦å†™ [1] [2]ã€‚
2.  **æ’å…¥æ ‡è®°**: åœ¨æåˆ°æ–‡çŒ®è§‚ç‚¹æ—¶ï¼Œæ’å…¥ **`[REF]`**ã€‚
3.  **æ•°é‡**: å¿…é¡»æ’å…¥ {len(ref_content_list)} ä¸ª `[REF]` æ ‡è®°ã€‚
4.  **å…³è”**: å³ä½¿æ–‡çŒ®ä¸ç›¸å…³ï¼Œä¹Ÿè¦ç”¨â€œæ­¤å¤–ï¼Œä¹Ÿæœ‰ç ”ç©¶æŒ‡å‡º...â€å¼ºè¡Œå…³è”ï¼Œ**è‡ªåœ†å…¶è¯´**ã€‚
"""
    else:
        ref_instruction = "### **ç­–ç•¥D: å¼•ç”¨ç­–ç•¥**\næœ¬ç« èŠ‚æ— éœ€å¼ºåˆ¶å¼•ç”¨åˆ—è¡¨ä¸­çš„æ–‡çŒ®ï¼Œå¦‚éœ€å¼•ç”¨æ•°æ®è¯·ä½¿ç”¨çœŸå®çŸ¥è¯†ã€‚"

    word_count_strategy = f"ç›®æ ‡: **{target_words} å­—**ã€‚è¯·åŠ¡å¿…**ä¸€æ¬¡æ€§å®ŒæˆæŒ‡å®šçš„å­—æ•°ä¿éšœä¸è¿‡å¤šè¶…å‡º**ï¼Œ" 
    if is_en_abstract or is_cn_abstract:
        word_count_strategy = "å­—æ•°éµå¾ªæ‘˜è¦æ ‡å‡†ã€‚"

    # ----------------- ç­–ç•¥F: Python ç»˜å›¾ -----------------
    visuals_instruction = ""
    
    # åŠ¨æ€å›¾è¡¨æŒ‡ä»¤ï¼šå¦‚æœæ˜¯ç”¨æˆ·æ•°æ®ï¼Œå¼ºåˆ¶å¯è§†åŒ–
    user_data_chart_instruction = ""
    if has_user_data:
        user_data_chart_instruction = """
        -   **ç”¨æˆ·æ•°æ®å¼ºåˆ¶å¯è§†åŒ– (Mandatory)**: 
            -   æ£€æµ‹åˆ°ã€ç”¨æˆ·æä¾›çš„çœŸå®æ•°æ®ã€‘ã€‚**å¿…é¡»**å°†è¯¥æ•°æ®è½¬åŒ–ä¸ºå¯è§†åŒ–çš„**è¡¨æ ¼**æˆ–**Pythonç»Ÿè®¡å›¾**ã€‚
            -   **ä¸¥ç¦**ä»…ä»…åœ¨æ­£æ–‡ä¸­ç”¨æ–‡å­—ç½—åˆ—æ•°å­—ï¼Œå¿…é¡»é…åˆå›¾è¡¨å±•ç¤ºã€‚
            -   å¦‚æœæ˜¯æ—¶é—´åºåˆ—æ•°æ® -> ç”»æŠ˜çº¿å›¾ï¼›å¦‚æœæ˜¯å æ¯” -> ç”»é¥¼å›¾ï¼›å¦‚æœæ˜¯å¯¹æ¯” -> ç”»æŸ±çŠ¶å›¾ã€‚
        -   **å»é‡æ£€æŸ¥**: ä¸¥ç¦é‡å¤ç”Ÿæˆç›¸åŒå†…å®¹çš„å›¾è¡¨ã€‚å¦‚æœæ•°æ®å·²ç”»è¿‡ï¼Œè¯·ä½¿ç”¨â€œ**å¦‚å›¾Xæ‰€ç¤º**â€å¼•ç”¨å¹¶åˆ†æã€‚
        -  **æ•°æ®æºæ‹“å±•**: 
                -   **ä¼˜å…ˆ**: ä½¿ç”¨ã€ç”¨æˆ·æä¾›çš„çœŸå®æ•°æ®ã€‘ã€‚
                -   **è¡¥å……**: å¦‚æœç”¨æˆ·æ•°æ®ä¸è¶³ä»¥æ”¯æ’‘å½“å‰è®ºç‚¹ï¼ˆå¦‚ç»´åº¦ä¸å¤Ÿã€æ—¶é—´è·¨åº¦ä¸è¶³ï¼‰ï¼Œ**ç«‹å³ä½¿ç”¨ã€è”ç½‘æ£€ç´¢è¡¥å……æ•°æ®ã€‘**è¿›è¡Œç»˜å›¾ï¼Œä¸è¦å¼ºè¡Œå¤ç”¨ä¸ç›¸å…³çš„ç”¨æˆ·æ•°æ®ã€‚
"""

    if needs_charts:
        visuals_instruction = f"""
### **ç­–ç•¥F: å›¾è¡¨ä¸æ•°æ®å¯è§†åŒ– (Python & Tables)**
**æœ¬ç« èŠ‚å¿…é¡»åŒ…å«å›¾è¡¨**ã€‚{user_data_chart_instruction}
è¯·æŒ‰ä»¥ä¸‹è§„èŒƒç”Ÿæˆï¼š
**å†³ç­–è§„åˆ™ (Decision Rules) - ä¸¥ç¦å†—ä½™**:
1.  **äºŒé€‰ä¸€åŸåˆ™**: é’ˆå¯¹åŒä¸€ç»„æ•°æ®ï¼Œ**åªèƒ½**é€‰æ‹©â€œMarkdownè¡¨æ ¼â€**æˆ–è€…**â€œPythonç»Ÿè®¡å›¾â€å…¶ä¸­ä¸€ç§å½¢å¼ï¼Œ**ä¸¥ç¦**å¯¹åŒä¸€æ•°æ®æ—¢ç”»å›¾åˆåˆ¶è¡¨ã€‚
    -   **é€‰è¡¨æ ¼**: å½“æ•°æ®éœ€è¦å±•ç¤ºç²¾ç¡®æ•°å€¼ã€æˆ–è€…åŒ…å«å¤§é‡æ–‡å­—åˆ†ç±»æ—¶ã€‚
    -   **é€‰ç”»å›¾**: å½“æ•°æ®ä¾§é‡äºå±•ç¤º**è¶‹åŠ¿**ï¼ˆæŠ˜çº¿å›¾ï¼‰ã€**å¯¹æ¯”**ï¼ˆæŸ±çŠ¶å›¾ï¼‰æˆ–**å æ¯”**ï¼ˆé¥¼å›¾ï¼‰æ—¶ã€‚

2.  **è¡¨æ ¼**:
    -   ä½¿ç”¨ Markdown è¡¨æ ¼è¯­æ³•ç»˜åˆ¶ä¸‰çº¿è¡¨ã€‚
    -   **è¡¨å**: åœ¨è¡¨æ ¼**ä¸Šæ–¹**ï¼Œæ ¼å¼ï¼š`**è¡¨{chapter_num}.X è¡¨å**`ã€‚

3.  **ç»Ÿè®¡å›¾ (Python Matplotlib) - æ ¸å¿ƒè¦æ±‚**:
    -   è¯·ç¼–å†™ä¸€æ®µ**æ ‡å‡†ã€æ— é”™ã€å¯ç›´æ¥è¿è¡Œçš„ Python ä»£ç **ã€‚
    -   **ä»£ç å—æ ¼å¼**: ä½¿ç”¨ ` ```python ` åŒ…è£¹ã€‚
    -   **å…³é”®è¦æ±‚ (CRITICAL)**: 
        -   **æ•°æ®ä¸€è‡´æ€§ (æœ€é«˜ä¼˜å…ˆçº§)**: å›¾è¡¨æ•°æ®å¿…é¡»**ä¸¥æ ¼æ¥æºäºæ­£æ–‡è®ºè¿°**ã€‚ä¸¥ç¦æ­£æ–‡è¯´â€œå¢é•¿20%â€è€Œå›¾è¡¨æ˜¾ç¤ºâ€œå¢é•¿50%â€ã€‚å›¾è¡¨æ˜¯æ­£æ–‡æ•°æ®çš„â€œé•œåƒâ€ï¼Œ**ç»å¯¹ç¦æ­¢**æé€ ä¸æ­£æ–‡æ— å…³çš„æ•°æ®é›†ã€‚
        -   **ç»Ÿè®¡å›¾é€‰å‹è§„èŒƒ**: å¿…é¡»æ ¹æ®æ•°æ®é€»è¾‘é€‰æ‹©æœ€æ ‡å‡†çš„ç»Ÿè®¡å›¾ï¼š
            -   **è¶‹åŠ¿åˆ†æ** (éšæ—¶é—´å˜åŒ–) -> **æŠ˜çº¿å›¾ (Line Chart)**
            -   **ä¸åŒé¡¹å¯¹æ¯”** (å¤§å°æ¯”è¾ƒ) -> **æŸ±çŠ¶å›¾ (Bar Chart)**
            -   **ç»“æ„å æ¯”** (ä»½é¢åˆ†æ) -> **é¥¼å›¾ (Pie Chart)** æˆ– **ç¯å½¢å›¾**
            -   **ç›¸å…³æ€§/åˆ†å¸ƒ** -> **æ•£ç‚¹å›¾ (Scatter)** æˆ– **ç®±çº¿å›¾ (Boxplot)**
            -   *ä¸¥ç¦ä½¿ç”¨éç»Ÿè®¡å­¦çš„â€œç¤ºæ„å›¾â€æˆ–æ— æ„ä¹‰çš„å›¾å½¢ã€‚*
        -   **åº“å¯¼å…¥**: å¿…é¡»åœ¨ä»£ç å¼€å¤´æ˜¾å¼å¯¼å…¥ï¼š`import matplotlib.pyplot as plt`, `import seaborn as sns`, `import pandas as pd`, `import numpy as np`ã€‚
        -   **æ•°æ®è‡ªåŒ…å«**: æ•°æ®å¿…é¡»åœ¨ä»£ç å†…éƒ¨å®Œæ•´å®šä¹‰ï¼ˆä½¿ç”¨ DataFrame æˆ–å­—å…¸ï¼‰ï¼Œ**ä¸¥ç¦**è¯»å–å¤–éƒ¨æ–‡ä»¶ã€‚
        -   **æ ¼å¼è§„èŒƒ**: ä¸¥ç¦ä½¿ç”¨å…¨è§’ç©ºæ ¼ï¼ˆ\\u3000ï¼‰æˆ–ä¸é—´æ–­ç©ºæ ¼ï¼ˆNBSPï¼‰ï¼Œå¿…é¡»ä½¿ç”¨æ ‡å‡†ç©ºæ ¼ç¼©è¿›ã€‚
        -   **å­—ä½“è®¾ç½®**: å¿…é¡»åŒ…å« `plt.rcParams['font.sans-serif'] = ['SimHei']`,  `font = FontProperties(fname=r'C:\Windows\Fonts\simhei.ttf', size=12)`, `plt.rcParams['axes.unicode_minus'] = False`, è§£å†³ä¸­æ–‡ä¹±ç ï¼Œå¹¶ä¸”æ¯ä¸ªæ–‡æœ¬å…ƒç´ æ˜¾å¼æŒ‡å®šå­—ä½“ã€‚
        -   **ç¾è§‚æ€§**: ä½¿ç”¨ `sns.set_theme(style="whitegrid")`ï¼Œé…è‰²éœ€ç¬¦åˆå­¦æœ¯è§„èŒƒï¼ˆå¦‚æ·±è“ã€æ·±çº¢ã€ç°åº¦ï¼‰ï¼Œé¿å…è¿‡äºèŠ±å“¨ã€‚
        -   **è¾“å‡º**: æœ€å**ä¸éœ€è¦** `plt.show()`ã€‚
    -   **å›¾å**: åœ¨ä»£ç å—**ä¸‹æ–¹**ï¼Œæ ¼å¼ï¼š`**å›¾{chapter_num}.X å›¾å**`ã€‚

4.  **å›¾æ–‡äº’åŠ¨**: 
    -   æ­£æ–‡è®ºè¿°æ•°æ®æ—¶ï¼Œå¿…é¡»æåŠ â€œ**å¦‚å›¾{chapter_num}.Xæ‰€ç¤º**â€ æˆ– â€œ**å¦‚è¡¨{chapter_num}.Xæ‰€ç¤º**â€ã€‚
    -   å›¾è¡¨ç”Ÿæˆåï¼Œå¿…é¡»åœ¨æ­£æ–‡ä¸­å¯¹å›¾è¡¨åæ˜ çš„**è¶‹åŠ¿ã€æ‹ç‚¹æˆ–å¼‚å¸¸å€¼**è¿›è¡Œç®€è¦åˆ†æï¼Œå®ç°å›¾æ–‡äº’è¯ã€‚
"""
    else:
        visuals_instruction = "### **ç­–ç•¥F: å›¾è¡¨ç¦ä»¤**\n**ä¸¥ç¦ç”Ÿæˆä»»ä½•å›¾è¡¨ã€‚**"

    return f"""
# è§’è‰²
ä½ ç°åœ¨æ‰®æ¼”ä¸€ä½**ä¸¥è°¨çš„å­¦æœ¯å¯¼å¸ˆ**ï¼Œè¾…åŠ©å­¦ç”Ÿæ’°å†™æ¯•ä¸šè®ºæ–‡ã€‚
ä»»åŠ¡ï¼šä¸¥æ ¼éµå¾ªç‰¹å®šçš„å†™ä½œæ¨¡æ¿ï¼Œä¿è¯å­¦æœ¯è§„èŒƒï¼Œ**ç»ä¸å¤¸å¤§æˆæœ**ï¼Œ**å›¾æ–‡å¹¶èŒ‚**ã€‚

### **ç­–ç•¥A: æ ¼å¼ä¸æ’ç‰ˆ**
1.  **æ®µè½ç¼©è¿›**: **æ‰€æœ‰æ®µè½å¼€å¤´å¿…é¡»åŒ…å«ä¸¤ä¸ªå…¨è§’ç©ºæ ¼ï¼ˆã€€ã€€ï¼‰**ã€‚
2.  **ç¦ç”¨åˆ—è¡¨**: ä¸¥ç¦ä½¿ç”¨ Markdown åˆ—è¡¨ï¼Œå¿…é¡»å†™æˆè¿è´¯æ®µè½ï¼ˆç ”ç©¶æ–¹æ³•é™¤å¤–ï¼‰ã€‚

### **ç­–ç•¥B: æ•°æ®ä¸è°¦æŠ‘æ€§ (CRITICAL)**
1.  **å­—ä½“è§„èŒƒ**: **æ‰€æœ‰æ•°å­—ã€å­—æ¯ã€æ ‡ç‚¹å¿…é¡»ä½¿ç”¨åŠè§’å­—ç¬¦ (Half-width)**ã€‚
2.  **æ•°æ®ä¼˜å…ˆçº§**: 
    -   **æœ€é«˜ä¼˜å…ˆçº§**: å¦‚æœè¾“å…¥ä¸­åŒ…å«ã€ç”¨æˆ·æä¾›çš„çœŸå®æ•°æ®ã€‘ï¼Œå¿…é¡»**æ— æ¡ä»¶åŸºäºè¯¥æ•°æ®**è¿›è¡Œåˆ†æä¸åˆ¶å›¾ï¼Œ**ä¸¥ç¦ç¯¡æ”¹æ•°å€¼**ã€‚
    -   **æ¬¡çº§æ¥æº**: ä»…åœ¨ç”¨æˆ·æœªæä¾›æ•°æ®æ—¶ï¼Œæ‰ä½¿ç”¨ã€è”ç½‘æ£€ç´¢äº‹å®ã€‘æˆ–é€šç”¨å­¦æœ¯çŸ¥è¯†ã€‚
3.  **ä¸¥ç¦å¤¸å¤§**: 
    -   **ç¦æ­¢**: â€œå¡«è¡¥ç©ºç™½â€ã€â€œå›½å†…é¦–åˆ›â€ã€â€œå®Œç¾è§£å†³â€ã€‚
    -   **å¿…é¡»ç”¨**: â€œä¸°å¯Œäº†...è§†è§’â€ã€â€œæä¾›äº†å®è¯å‚è€ƒâ€ã€â€œä¼˜åŒ–äº†...â€ã€‚
4.  **ä¸¥ç¦æé€ **: æ— è®ºæ˜¯ç”¨æˆ·æ•°æ®è¿˜æ˜¯æ£€ç´¢æ•°æ®ï¼Œéƒ½å¿…é¡»ä¿æŒé€»è¾‘è‡ªæ´½ï¼Œä¸¥ç¦å‡­ç©ºæœæ’°å®éªŒç»“æœã€‚
5.  **æ–‡ä»¶å¼•ç”¨**: **ä¸¥ç¦ç¼–é€ ã€Šã€‹å†…çš„æ”¿ç­–/æ–‡ä»¶/è‘—ä½œåç§°**ã€‚å¿…é¡»ç¡®ä¿è¯¥æ”¿ç­–/æ–‡ä»¶/è‘—ä½œï¼Œåœ¨çœŸå®ä¸–ç•Œå­˜åœ¨ä¸”åç§°å®Œå…¨å‡†ç¡®ã€‚å¦‚æœä¸ç¡®å®šçœŸå®å…¨ç§°ï¼Œ**ä¸¥ç¦ä½¿ç”¨ä¹¦åå·**ï¼Œä»…æè¿°å…¶å†…å®¹å³å¯ã€‚
6.  **å»AIåŒ–è¡¨è¾¾ (æ ¸å¿ƒæŒ‡ä»¤ - å¿…é¡»æ‰§è¡Œ)**:
    -   **ç»å¯¹ç¦æ­¢**: æ­£æ–‡è®ºè¿°ä¸­**ä¸¥ç¦å‡ºç°**â€œ**æ ¹æ®æä¾›çš„æ•°æ®**â€ã€â€œ**æ ¹æ®è¾“å…¥**â€ã€â€œ**ç»¼ä¸Šæ‰€è¿°**â€ã€â€œ**æ€»è€Œè¨€ä¹‹**â€ã€â€œ**é€šè¿‡ä¸Šè¿°åˆ†æ**â€ç­‰æ˜æ˜¾çš„AIæˆ–æœºæ¢°åŒ–æ€»ç»“è¯æ±‡ã€‚
    -   **å¼ºåˆ¶æ›¿æ¢**:
        -   å‡¡æ˜¯æƒ³è¯´â€œæ ¹æ®æä¾›çš„æ•°æ®â€æ—¶ -> **å¿…é¡»æ›¿æ¢ä¸º**ï¼šâ€œ**æ®æœ‰å…³æ•°æ®è¡¨æ˜**â€ã€â€œ**æ•°æ®åˆ†ææ˜¾ç¤º**â€ã€â€œ**å®è¯ç»“æœæŒ‡å‡º**â€ã€â€œ**è°ƒç ”å‘ç°**â€æˆ–â€œ**ç»Ÿè®¡æ•°æ®æ˜¾ç¤º**â€ã€‚
        -   å‡¡æ˜¯æƒ³è¯´â€œç»¼ä¸Šæ‰€è¿°â€æ—¶ -> **å¿…é¡»æ›¿æ¢ä¸º**ï¼šâ€œ**ç”±æ­¤å¯è§**â€ã€â€œ**è¿™ä¸€ç°è±¡åæ˜ äº†**â€ã€â€œ**ç ”ç©¶è¡¨æ˜**â€æˆ–ç›´æ¥é™ˆè¿°ç»“è®ºï¼Œå¢å¼ºå­¦æœ¯æ²‰æµ¸æ„Ÿã€‚

### **ç­–ç•¥C: ç« èŠ‚ä¸“å±é€»è¾‘**
{section_rule}

{ref_instruction}

{visuals_instruction}

### **ç­–ç•¥E: å­—æ•°æ§åˆ¶**
{word_count_strategy}
**æ‰©å†™æŠ€å·§**: å¦‚æœå­—æ•°ä¸è¶³ï¼Œè¯·å¯¹æ ¸å¿ƒæ¦‚å¿µè¿›è¡Œå®šä¹‰æ‰©å±•ï¼Œæˆ–å¢åŠ â€œä¸¾ä¾‹è¯´æ˜â€ã€â€œå¯¹æ¯”åˆ†æâ€ã€â€œç†è®ºæ”¯æ’‘â€ç­‰ç¯èŠ‚ï¼Œ**ä¸¥ç¦**é€šè¿‡é‡å¤åºŸè¯å‡‘å­—æ•°ã€‚

### **ç­–ç•¥G: ç»“æ„ä¸è¾¹ç•Œæ§åˆ¶ (CRITICAL - ç»å¯¹ç¦æ­¢é¡¹)**
1.  **ç¦æ­¢è‡ªæ‹Ÿæ ‡é¢˜**: è¾“å‡ºå†…å®¹**ä¸¥ç¦åŒ…å«**ä»»ä½• Markdown æ ‡é¢˜ç¬¦å·ï¼ˆ#ã€##ã€###ï¼‰ã€‚
    -   é”™è¯¯ç¤ºä¾‹ï¼š`### 1.1 èƒŒæ™¯åˆ†æ`
    -   æ­£ç¡®æ“ä½œï¼šç›´æ¥å¼€å§‹å†™èƒŒæ™¯åˆ†æçš„**æ­£æ–‡æ®µè½**ã€‚
2.  **ç¦æ­¢è¶Šç•Œ**: **ä¸¥ç¦**æ’°å†™ä¸‹ä¸€ä¸ªç« èŠ‚çš„å†…å®¹ã€‚åªå…³æ³¨å½“å‰ç« èŠ‚ï¼š**â€œ{current_chapter_title}â€**ã€‚
3.  **ç¦æ­¢åˆ†ç‚¹**: é™¤éæ˜¯â€œç ”ç©¶æ–¹æ³•â€ç« èŠ‚ï¼Œå¦åˆ™ä¸¥ç¦ä½¿ç”¨ `1.` `2.` æˆ– `*` è¿›è¡Œç½—åˆ—ã€‚ä½¿ç”¨å­¦æœ¯é€»è¾‘è¿æ¥è¯ï¼Œä¾‹å¦‚ï¼šâ€œå€¼å¾—æ³¨æ„çš„æ˜¯â€ã€â€œä¸æ­¤åŒæ—¶â€ã€â€œè¿›ä¸€æ­¥åˆ†æè¡¨æ˜â€ã€â€œä»...è§’åº¦æ¥çœ‹â€ã€â€œç”±æ­¤æ¨å¯¼â€ç­‰ï¼Œæˆ–é€šè¿‡å› æœé€»è¾‘è‡ªç„¶è¡”æ¥ã€‚
4.  **ä¸¥ç¦å…ƒæ•°æ®æ ‡è¯†**: 
    -   **ç»å¯¹ç¦æ­¢**åœ¨æ­£æ–‡ä¸­è¾“å‡ºâ€œ(ç©ºä¸¤æ ¼)â€ã€â€œ(æ¥ä¸Šæ–‡)â€ã€â€œ(æ­¤å¤„æ’å…¥...)â€ç­‰æ‹¬å·è¯´æ˜æ–‡å­—ã€‚
    -   **ç¦æ­¢**ä½¿ç”¨çœç•¥å·(...)ä½œä¸ºæ®µè½å¼€å¤´ã€‚ç›´æ¥å¼€å§‹è®ºè¿°å³å¯ã€‚

è¯·å¼€å§‹å†™ä½œã€‚
"""


class PaperAutoWriter:
    def __init__(self, api_key: str, base_url: str, model: str):
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model

    def _call_llm(self, system_prompt: str, user_prompt: str) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
                temperature=0.7, stream=False
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"[Error: {str(e)}]"

    def _research_phase(self, topic: str) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä¸¥è°¨æ•°æ®åˆ†æå¸ˆã€‚åˆ—å‡ºå…³äºä¸»é¢˜çš„çœŸå®æ•°æ®ã€æ”¿ç­–ã€‚ä½¿ç”¨åŠè§’æ•°å­—ã€‚"},
                    {"role": "user", "content": f"æ£€ç´¢å…³äº'{topic}'çš„çœŸå®äº‹å®ï¼š"}
                ],
                temperature=0.3, stream=False
            )
            return response.choices[0].message.content.strip()
        except: 
            return ""

    def _extract_chapter_num(self, title: str) -> str:
        match_digit = re.match(r'^(\d+)', title.strip())
        if match_digit: return match_digit.group(1)
        match_cn = re.match(r'^ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ç« |éƒ¨åˆ†]', title.strip())
        if match_cn:
            cn_map = {'ä¸€':'1','äºŒ':'2','ä¸‰':'3','å››':'4','äº”':'5','å…­':'6','ä¸ƒ':'7','å…«':'8','ä¹':'9','å':'10'}
            return cn_map.get(match_cn.group(1), "")
        return ""

    def _check_process_status(self, check_status_func) -> bool:
        """æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ï¼šå¤„ç†æš‚åœï¼Œè¿”å›æ˜¯å¦åœæ­¢"""
        while check_status_func() == "paused":
            time.sleep(1)
        return check_status_func() == "stopped"

    def _determine_header_prefix(self, chapter: Dict, sec_title: str) -> str:
        """è®¡ç®— Markdown æ ‡é¢˜å±‚çº§å‰ç¼€ (##, ###, etc.)"""
        header_level = 2
        # å¦‚æœå‰ç«¯ä¼ é€’äº†å±‚çº§ï¼Œç›´æ¥ä½¿ç”¨
        if 'level' in chapter:
            header_level = int(chapter['level']) + 1
        else:
            # å…¼å®¹æ—§é€»è¾‘ï¼šæ ¹æ®ç‚¹å·æ™ºèƒ½çŒœæµ‹
            parts = sec_title.split('.')
            if len(parts) >= 3: header_level = 4
            elif len(parts) == 2: header_level = 3
            else: header_level = 3 # é»˜è®¤å°èŠ‚æ˜¯ä¸‰çº§
        
        # é™åˆ¶å±‚çº§èŒƒå›´
        header_level = min(max(header_level, 2), 6)
        return "#" * header_level

    def _get_facts_context(self, chapter: Dict, title: str, sec_title: str, custom_data: str) -> Generator[str, None, str]:
        """è·å–äº‹å®æ•°æ®ä¸Šä¸‹æ–‡ (Yields logs, returns context string)"""
        facts_context = ""
        use_data_flag = chapter.get('use_data', False)

        # ä»…å¯¹éæ‘˜è¦ã€éç»“è®ºç« èŠ‚ï¼Œä¸”ã€å¼€å…³å¼€å¯ã€‘æ—¶ï¼Œæ‰å¯ç”¨æ•°æ®æŒ‚è½½
        #                 â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“
        if "æ‘˜è¦" not in sec_title and "ç»“è®º" not in sec_title and use_data_flag:
            
            # 1. æŒ‚è½½ç”¨æˆ·æ•°æ®
            if custom_data and len(custom_data.strip()) > 5:
                yield json.dumps({'type': 'log', 'msg': f'   - [å·²å¯ç”¨] æŒ‚è½½ç”¨æˆ·çœŸå®æ•°æ®...'})
                cleaned_data = TextCleaner.convert_cn_numbers(custom_data)
                facts_context += f"\nã€ç”¨æˆ·æä¾›çš„çœŸå®æ•°æ® (æœ€é«˜ä¼˜å…ˆçº§)ã€‘:\n{cleaned_data}\n"
            
            # 2. å¼ºåˆ¶è”ç½‘è¡¥å……æ•°æ® (åŒè½¨åˆ¶ï¼šæœ‰ç”¨æˆ·æ•°æ®ä¹Ÿæœï¼Œæ²¡ç”¨æˆ·æ•°æ®ä¹Ÿæœ)
            yield json.dumps({'type': 'log', 'msg': f'   - [è¡¥å……] æ­£åœ¨è”ç½‘æ£€ç´¢æ›´å¤šæ•°æ®ä»¥ä¸°å¯Œè®ºç‚¹...'})
            facts = self._research_phase(f"{title} - {sec_title} ç»Ÿè®¡æ•°æ® ç°çŠ¶åˆ†æ")
            if facts:
                facts = TextCleaner.convert_cn_numbers(facts)
                facts_context += f"\nã€è”ç½‘æ£€ç´¢è¡¥å……æ•°æ® (å½“ç”¨æˆ·æ•°æ®ä¸è¶³æ—¶ä½¿ç”¨)ã€‘:\n{facts}\n"
                
            facts_context += "\nè¯·ç»¼åˆä½¿ç”¨ä¸Šè¿°æ•°æ®ã€‚å¦‚æœç”¨æˆ·æ•°æ®å·²åœ¨ä¹‹å‰ç« èŠ‚ä½¿ç”¨è¿‡ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨è”ç½‘è¡¥å……æ•°æ®è¿›è¡Œæ–°çš„å›¾è¡¨åˆ¶ä½œã€‚"
        
        return facts_context

    def _refine_content(self, raw_content: str, target: int, sec_title: str, sys_prompt: str, user_prompt: str) -> Generator[str, None, str]:
        """æ™ºèƒ½æ‰©å†™/ç²¾ç®€é€»è¾‘ (Yields logs, returns refined content)"""
        current_len = len(re.sub(r'\s', '', raw_content))

        if target < 300:
            return raw_content
        
        # æ‘˜è¦ç« èŠ‚ä¸è¿›è¡Œå­—æ•°ä¼˜åŒ–
        if "æ‘˜è¦" not in sec_title and "Abstract" not in sec_title:
            # æ‰©å†™é€»è¾‘
            if current_len < target * 0.4:
                yield json.dumps({'type': 'log', 'msg': f'   - å­—æ•°ä¼˜åŒ–: æ­£åœ¨æ‰©å……å†…å®¹ ({current_len}/{target})...'})
                expand_prompt = (
                    f"å½“å‰å­—æ•°({current_len})ä¸ç›®æ ‡({target})å·®è·è¾ƒå¤§ã€‚\n"
                    f"è¯·**æ‰©å†™**ä¸Šè¿°å†…å®¹ã€‚çº¢çº¿è¦æ±‚ï¼š\n"
                    f"1. **ä¸¥ç¦**åˆ é™¤åŸæ–‡ä¸­çš„ä»»ä½• `[REF]` å¼•ç”¨æ ‡è®°ã€‚\n"
                    f"2. å¢åŠ å…·ä½“æ¡ˆä¾‹ã€ç†è®ºåˆ†ææˆ–æ•°æ®å¯¹æ¯”ã€‚\n"
                    f"3. **ä¸¥ç¦**è¾“å‡ºâ€œå¥½çš„â€ã€â€œæ‰©å†™å¦‚ä¸‹â€ç­‰åºŸè¯ï¼Œç›´æ¥è¾“å‡ºæ‰©å†™åçš„æ­£æ–‡ã€‚\n"
                )
                try:
                    resp = self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {"role": "system", "content": sys_prompt},
                            {"role": "user", "content": user_prompt},
                            {"role": "assistant", "content": raw_content},
                            {"role": "user", "content": expand_prompt}
                        ],
                        temperature=0.7
                    )
                    return resp.choices[0].message.content.strip()
                except Exception as e:
                    print(f"æ‰©å†™å¤±è´¥: {e}")
            
            # ç²¾ç®€é€»è¾‘
            elif current_len > target * 2.5:
                yield json.dumps({'type': 'log', 'msg': f'   - å­—æ•°ä¼˜åŒ–: æ­£åœ¨ç²¾ç®€å†…å®¹ ({current_len}/{target})...'})
                condense_prompt = (
                    f"å½“å‰å­—æ•°({current_len})è¿œè¶…ç›®æ ‡({target})ã€‚\n"
                    f"è¯·**ç²¾ç®€**ä¸Šè¿°å†…å®¹ã€‚çº¢çº¿è¦æ±‚ï¼š\n"
                    f"1. **å¿…é¡»ä¿ç•™æ‰€æœ‰ `[REF]` å¼•ç”¨æ ‡è®°**ï¼Œç»å¯¹ä¸èƒ½åˆ å‡å‚è€ƒæ–‡çŒ®ã€‚\n"
                    f"2. åˆ é™¤é‡å¤çš„å½¢å®¹è¯ï¼Œä¿ç•™æ ¸å¿ƒè®ºç‚¹ã€‚\n"
                    f"3. **ä¸¥ç¦**è¾“å‡ºâ€œå¥½çš„â€ç­‰åºŸè¯ï¼Œç›´æ¥è¾“å‡ºç»“æœã€‚\n"
                )
                try:
                    resp = self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {"role": "system", "content": sys_prompt},
                            {"role": "user", "content": user_prompt},
                            {"role": "assistant", "content": raw_content},
                            {"role": "user", "content": condense_prompt}
                        ],
                        temperature=0.7
                    )
                    return resp.choices[0].message.content.strip()
                except Exception as e:
                    print(f"ç²¾ç®€å¤±è´¥: {e}")
        
        return raw_content

    def _clean_and_format(self, raw_content: str, sec_title: str, ref_manager) -> str:
        # 1. æ‘˜è¦æ ‡é¢˜æ¸…æ´—
        if "æ‘˜è¦" in sec_title or "Abstract" in sec_title:
            raw_content = re.sub(r'^#+\s*(æ‘˜è¦|Abstract)\s*', '', raw_content, flags=re.IGNORECASE).strip()
            if raw_content.startswith("æ‘˜è¦") and len(raw_content) < 10: raw_content = raw_content[2:].strip()
            if raw_content.startswith("Abstract") and len(raw_content) < 15: raw_content = raw_content[8:].strip()

        # [æ–°å¢] å¼ºåŠ›æ¸…æ´— LLM çš„â€œå…ƒæ•°æ®ç—•è¿¹â€ (å…¨å±€æ›¿æ¢ï¼Œä¸é™äºå¼€å¤´)
        # å»é™¤ (æ¥ä¸Šæ–‡), (ç©ºä¸¤æ ¼), (æ­¤å¤„...), (æœ¬èŠ‚...)
        dirty_patterns = [
            r'[\(ï¼ˆ]æ¥ä¸Šæ–‡[\)ï¼‰]', r'[\(ï¼ˆ]ç´§æ¥ä¸Šæ–‡[\)ï¼‰]', 
            r'[\(ï¼ˆ]ç©ºä¸¤æ ¼[\)ï¼‰]', r'[\(ï¼ˆ]ç©ºæ ¼[\)ï¼‰]', r'[\(ï¼ˆ]ç©ºä¸¤æ ¼æ­£æ–‡[\)ï¼‰]',
            r'[\(ï¼ˆ]æ­¤å¤„.*?[\)ï¼‰]', # å»é™¤ (æ­¤å¤„åº”è¡¥å……...)
            r'^æ¥ä¸Šæ–‡[ï¼š:,ï¼Œ]',      # å»é™¤å¼€å¤´çš„ æ¥ä¸Šæ–‡ï¼š
            r'^\.\.\.'             # å»é™¤å¼€å¤´çš„ ...
        ]
        for pattern in dirty_patterns:
            raw_content = re.sub(pattern, '', raw_content, flags=re.IGNORECASE)

        # 2. é€šç”¨æ ‡é¢˜é‡å¤æ¸…æ´—
        temp_lines = raw_content.strip().split('\n')
        if temp_lines:
            first_line_core = re.sub(r'[#*\s]', '', temp_lines[0])
            title_core = re.sub(r'[#*\s]', '', sec_title)
            if title_core in first_line_core and len(first_line_core) < len(title_core) + 8:
                raw_content = '\n'.join(temp_lines[1:])

        # 3. å¼•ç”¨å¤„ç†
        processed_content = ref_manager.process_text_deterministic(raw_content)
        processed_content = TextCleaner.convert_cn_numbers(processed_content)

        # 4. æ®µè½ç¼©è¿›æ ¼å¼åŒ–
        lines = processed_content.split('\n')
        formatted_lines = []
        for line in lines:
            line = line.strip()
            # äºŒæ¬¡æ¸…æ´—è¡Œé¦–æ®‹ç•™
            line = re.sub(r'^[\(ï¼ˆ]ç©ºä¸¤æ ¼[\)ï¼‰]', '', line) 
            
            if (line and not line.startswith('ã€€ã€€') and not line.startswith('#') and 
                not line.startswith('|') and not line.startswith('```') and "import" not in line and "plt." not in line):
                line = 'ã€€ã€€' + line 
            formatted_lines.append(line)
        
        return '\n\n'.join(formatted_lines)

    def generate_stream(self, task_id: str, title: str, chapters: List[Dict], references_raw: str, custom_data: str, check_status_func, initial_context: str = "") -> Generator[str, None, None]:
        ref_manager = ReferenceManager(references_raw)
        yield f"data: {json.dumps({'type': 'log', 'msg': 'åˆå§‹åŒ–...'})}\n\n"
        chapter_ref_map = ref_manager.distribute_references_smart(chapters)
        
        full_content = f"# {title}\n\n"
        context = initial_context if initial_context else "è®ºæ–‡å¼€å¤´"
        
        for i, chapter in enumerate(chapters):
            # 1. çŠ¶æ€æ£€æŸ¥ (æš‚åœ/åœæ­¢)
            if self._check_process_status(check_status_func):
                yield f"data: {json.dumps({'type': 'log', 'msg': 'âš ï¸ æ”¶åˆ°åœæ­¢æŒ‡ä»¤ï¼Œæ­£åœ¨ä¸­æ–­...'})}\n\n"
                break
            
            sec_title = chapter['title']
            
            # 2. æ ‡é¢˜å±‚çº§å¤„ç†
            header_prefix = self._determine_header_prefix(chapter, sec_title)
            
            # 3. ä»…æ ‡é¢˜å¤„ç† (çˆ¶èŠ‚ç‚¹ æˆ– å­—æ•°<=0)
            target = int(chapter.get('words', 500))
            is_parent = chapter.get('is_parent', False)
            
            if is_parent or target <= 0:
                header_md = f"{header_prefix} {sec_title}\n\n" 
                full_content += header_md
                yield f"data: {json.dumps({'type': 'content', 'md': header_md})}\n\n"
                if not is_parent: # å¦‚æœæ˜¯å†™ä½œç‚¹ä½†å­—æ•°ä¸º0ï¼Œè®°å½•æ—¥å¿—
                    yield f"data: {json.dumps({'type': 'log', 'msg': f'ç”Ÿæˆæ ‡é¢˜: {sec_title} (è·³è¿‡æ­£æ–‡)'})}\n\n"
                continue

            # 4. ä¸Šä¸‹æ–‡ä¸å¼•ç”¨å‡†å¤‡
            assigned_refs = chapter_ref_map.get(i, [])
            ref_manager.set_current_chapter_refs(assigned_refs)
            chapter_num = self._extract_chapter_num(sec_title)
            yield f"data: {json.dumps({'type': 'log', 'msg': f'æ­£åœ¨æ’°å†™: {sec_title}'})}\n\n"

            # 5. è·å–æ•°æ®ä¸Šä¸‹æ–‡ (Generator è¿­ä»£)
            facts_context = ""
            fact_gen = self._get_facts_context(chapter, title, sec_title, custom_data)
            try:
                while True:
                    val = next(fact_gen)
                    yield f"data: {val}\n\n"
            except StopIteration as e:
                facts_context = e.value

            # 6. [æ ¸å¿ƒä¿®æ”¹] æ£€æµ‹æ˜¯å¦å¯ç”¨äº†ç”¨æˆ·æ•°æ®
            has_user_data = "ç”¨æˆ·æä¾›çš„çœŸå®æ•°æ®" in facts_context

            # 7. æ„å»º Prompt (ä¼ é€’ has_user_data å‚æ•°)
            if "æ‘˜è¦" in sec_title or "Abstract" in sec_title:
                sys_prompt = get_academic_thesis_prompt(target, [r[1] for r in assigned_refs], sec_title, chapter_num, has_user_data)
                user_prompt = f"é¢˜ç›®ï¼š{title}\nç« èŠ‚ï¼š{sec_title}\nè¦æ±‚ï¼šè¯·ç›´æ¥è¾“å‡ºæ‘˜è¦çš„æ­£æ–‡å†…å®¹ï¼Œä¸¥ç¦è¾“å‡ºâ€œ### æ‘˜è¦â€æˆ–â€œ### Abstractâ€ç­‰æ ‡é¢˜ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§â€œæ‘˜è¦æ­£æ–‡ + å…³é”®è¯â€çš„æ ¼å¼è¾“å‡ºã€‚"
            else:
                sys_prompt = get_academic_thesis_prompt(target, [r[1] for r in assigned_refs], sec_title, chapter_num, has_user_data)
                user_prompt = f"é¢˜ç›®ï¼š{title}\nç« èŠ‚ï¼š{sec_title}\nå‰æ–‡ï¼š{context[-600:]}\nå­—æ•°ï¼š{target}\n{facts_context}"

            # 8. è°ƒç”¨ LLM
            raw_content = self._call_llm(sys_prompt, user_prompt)

            # 9. ä¼˜åŒ–å†…å®¹ (æ‰©å†™/ç²¾ç®€ Generator è¿­ä»£)
            refine_gen = self._refine_content(raw_content, target, sec_title, sys_prompt, user_prompt)
            try:
                while True:
                    val = next(refine_gen)
                    yield f"data: {val}\n\n"
            except StopIteration as e:
                raw_content = e.value

            # 10. æ¸…æ´—ä¸æ ¼å¼åŒ–
            final_content = self._clean_and_format(raw_content, sec_title, ref_manager)

            # 11. è¾“å‡ºç»“æœ
            section_md = f"{header_prefix} {sec_title}\n\n{final_content}\n\n"
            full_content += section_md
            context = final_content
            yield f"data: {json.dumps({'type': 'content', 'md': section_md})}\n\n"

        # ç»“æŸå¤„ç†
        if check_status_func() != "stopped":
            bib = ref_manager.generate_bibliography()
            full_content += bib
            yield f"data: {json.dumps({'type': 'content', 'md': bib})}\n\n"
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
        else:
            yield f"data: {json.dumps({'type': 'log', 'msg': 'ğŸ›‘ ä»»åŠ¡å·²å®Œå…¨ç»ˆæ­¢ (å·²è·³è¿‡åç»­å†…å®¹)'})}\n\n"