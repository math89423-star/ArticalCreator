
import re
import json
import time
import base64
import concurrent.futures
from openai import OpenAI
from typing import Dict, List, Generator, Optional
from .reference import ReferenceManager
from .word import TextCleaner
from .prompts import get_rewrite_prompt, get_word_distribution_prompt, get_academic_thesis_prompt
from .word import MarkdownToDocx
try:
    from docx import Document
except ImportError:
    Document = None
try:
    from pypdf import PdfReader
except ImportError:
    PdfReader = None


class PaperAutoWriter:
    def __init__(self, api_key: str, base_url: str, model: str):
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        # ä¸»çº¿ç¨‹å®¢æˆ·ç«¯
        self.main_client = OpenAI(api_key=api_key, base_url=base_url, timeout=120.0)
    
    def _call_llm_with_client(self, client, system_prompt: str, user_prompt: str, images: list = None) -> str:
        """
        ä½¿ç”¨æŒ‡å®šçš„ client å®ä¾‹è°ƒç”¨ LLM
        æ”¯æŒå¯é€‰çš„ images å‚æ•° (List[dict])ï¼Œç”¨äºè§†è§‰æ¨¡å‹è¾“å…¥
        """
        # 1. æ„å»ºæ¶ˆæ¯ä½“
        messages = [{"role": "system", "content": system_prompt}]

        if images and len(images) > 0:
            # === å¤šæ¨¡æ€æ¶ˆæ¯æ„å»º (Multimodal) ===
            user_content = [{"type": "text", "text": user_prompt}]
            
            for img_file in images:
                try:
                    # è·å–æ–‡ä»¶ååç¼€ä»¥ç¡®å®š MIME type
                    filename = img_file.get('name', 'image.jpg').lower()
                    mime_type = "image/jpeg" # é»˜è®¤
                    if filename.endswith('.png'): mime_type = "image/png"
                    elif filename.endswith('.webp'): mime_type = "image/webp"
                    elif filename.endswith('.gif'): mime_type = "image/gif"
                    elif filename.endswith('.bmp'): mime_type = "image/bmp"

                    # è¯»å–æµå¹¶è½¬ä¸º base64
                    stream = img_file.get('content')
                    if stream:
                        stream.seek(0) # é‡ç½®æŒ‡é’ˆ
                        b64_str = base64.b64encode(stream.read()).decode('utf-8')
                        
                        user_content.append({
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:{mime_type};base64,{b64_str}"
                            }
                        })
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†å›¾ç‰‡å‡ºé”™: {e}")
            
            messages.append({"role": "user", "content": user_content})
        else:
            # === çº¯æ–‡æœ¬æ¶ˆæ¯æ„å»º ===
            messages.append({"role": "user", "content": user_prompt})

        # 2. å‘é€è¯·æ±‚
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model=self.model,
                    messages=messages, # ä½¿ç”¨æ„å»ºå¥½çš„ messages
                    temperature=0.7, 
                    stream=False
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                print(f"âš ï¸ [LLM Error] Attempt {attempt+1}/{max_retries}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œå‘ä¸ŠæŠ›å‡ºå¼‚å¸¸æˆ–è¿”å›ç©ºå­—ç¬¦ä¸²
                    # è¿™é‡Œé€‰æ‹©æŠ›å‡ºï¼Œä»¥ä¾¿ä¸Šå±‚æ•è·é”™è¯¯ä¿¡æ¯
                    raise e

    def _call_llm(self, system_prompt: str, user_prompt: str, images: list = None) -> str:
        """è°ƒç”¨æ–¹æ³• (å¢åŠ  images é€ä¼ )"""
        return self._call_llm_with_client(self.main_client, system_prompt, user_prompt, images=images)

    def _research_phase_with_client(self, client, topic: str) -> str:
        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    # [ä¿®æ”¹] å¼ºè°ƒæ—¶é—´èŒƒå›´ 2020-2025
                    {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸¥è°¨çš„æ•°æ®åˆ†æå¸ˆã€‚è¯·é‡ç‚¹æ£€ç´¢**è¿‘5å¹´ï¼ˆ2020-2025ï¼‰**çš„çœŸå®æ•°æ®ã€æœ€æ–°æ”¿ç­–å’Œè¡Œä¸šæŠ¥å‘Šã€‚å¿½ç•¥2019å¹´ä»¥å‰çš„è¿‡æ—¶ä¿¡æ¯ã€‚"},
                    {"role": "user", "content": f"æ£€ç´¢å…³äº'{topic}'çš„çœŸå®äº‹å®ï¼ˆå¿…é¡»æ˜¯2020å¹´ä»¥åçš„æ•°æ®ï¼‰ï¼š"}
                ],
                temperature=0.3, stream=False
            )
            return response.choices[0].message.content.strip()
        except: 
            return ""

    def _research_phase(self, topic: str) -> str:
        return self._research_phase_with_client(self.main_client, topic)

    def _check_process_status(self, check_status_func) -> bool:
        while check_status_func() == "paused":
            time.sleep(1)
        return check_status_func() == "stopped"

    def _extract_chapter_num(self, title: str) -> str:
        match = re.match(r'^(\d+)', title.strip())
        return match.group(1) if match else ""

    def _determine_header_prefix(self, chapter: Dict, sec_title: str) -> str:
        level = 2
        if 'level' in chapter: level = int(chapter['level']) + 1
        return "#" * min(max(level, 2), 6)

    def _clean_and_format(self, raw_content: str, sec_title: str, ref_manager) -> str:
        if "æ‘˜è¦" in sec_title or "Abstract" in sec_title:
            raw_content = re.sub(r'^#+\s*(æ‘˜è¦|Abstract)\s*', '', raw_content, flags=re.IGNORECASE).strip()
        dirty_patterns = [r'[\(ï¼ˆ]æ¥ä¸Šæ–‡[\)ï¼‰]', r'[\(ï¼ˆ]ç©ºä¸¤æ ¼[\)ï¼‰]', r'^\.\.\.', r'æ¥ä¸Šæ–‡ï¼š']
        for p in dirty_patterns:
            raw_content = re.sub(p, '', raw_content)
        if ref_manager:
            raw_content = ref_manager.process_text_deterministic(raw_content)
        processed = TextCleaner.convert_cn_numbers(raw_content)
        lines = []
        for line in processed.split('\n'):
            line = line.strip()
            if (line and not line.startswith('ã€€ã€€') and not line.startswith('#') and 
                not line.startswith('|') and not line.startswith('```') and "import" not in line):
                line = 'ã€€ã€€' + line 
            lines.append(line)
        return '\n\n'.join(lines)

    def _refine_content(self, raw_content: str, target: int, sec_title: str, sys_prompt: str, user_prompt: str) -> Generator[str, None, str]:
        # è®¡ç®—çº¯æ–‡æœ¬é•¿åº¦ï¼ˆæ’é™¤ä»£ç å—ï¼‰
        content_no_code = re.sub(r'```[\s\S]*?```', '', raw_content)
        current_len = len(re.sub(r'\s', '', content_no_code))
        # å¦‚æœç›®æ ‡å­—æ•°å¾ˆå°ï¼Œæˆ–è€…å½“å‰å­—æ•°å·²ç»è¾¾æ ‡ï¼ˆä¾‹å¦‚è¾¾åˆ°ç›®æ ‡çš„ 60%ï¼‰ï¼Œå°±ä¸å¤„ç†
        if target < 300 or current_len >= target * 0.6: 
            return raw_content
        # æ„å»ºæ‰©å†™æŒ‡ä»¤
        expand_prompt = user_prompt + f"\n\nã€ç³»ç»Ÿæ£€æµ‹ã€‘å½“å‰å­—æ•°ä»… {current_len} å­—ï¼Œè¿œä½äºç›®æ ‡ {target} å­—ã€‚è¯·åœ¨ä¿æŒåŸæœ‰è§‚ç‚¹çš„åŸºç¡€ä¸Šï¼Œå¤§å¹…æ‰©å……ç»†èŠ‚ã€å¢åŠ è®ºæ®ã€å±•å¼€ç†è®ºåˆ†æï¼Œç¡®ä¿å­—æ•°è¾¾æ ‡ã€‚"
        # å†æ¬¡è°ƒç”¨ LLM
        refined_content = self._call_llm(sys_prompt, expand_prompt)
        return refined_content

    def _fix_markdown_table_format(self, text):
        """
        å¼ºåŠ›ä¿®å¤è¡¨æ ¼æ ¼å¼
        1. è¯†åˆ«è¡¨æ ¼è¡Œï¼Œå¼ºåˆ¶å»é™¤ç¼©è¿› (é˜²æ­¢è¢«å½“åšä»£ç å—)
        2. ç¡®ä¿è¡¨æ ¼ä¸ä¸Šæ–¹æ–‡æœ¬ä¹‹é—´æœ‰ç©ºè¡Œ (Markdown æ ‡å‡†)
        """
        lines = text.split('\n')
        new_lines = []
        in_table = False
        
        for line in lines:
            # å…¼å®¹å…¨è§’ç©ºæ ¼çš„å»é™¤
            stripped = line.strip().replace('\u3000', '')
            # åˆ¤å®šæ˜¯å¦ä¸ºè¡¨æ ¼è¡Œ (ä»¥ | å¼€å¤´å¹¶ç»“å°¾)
            # å®½æ¾åŒ¹é…ï¼šåªè¦å»ç©ºåä»¥ | å¼€å¤´ä¸”åŒ…å«ç¬¬äºŒä¸ª | å³å¯
            is_table_row = stripped.startswith('|') and stripped.count('|') >= 2
            if is_table_row:
                if not in_table:
                    # æ£€æŸ¥ä¸Šä¸€è¡Œæ˜¯å¦ä¸ºç©ºï¼Œå¦‚æœä¸æ˜¯ï¼Œæ’å…¥ç©ºè¡Œ
                    if new_lines and new_lines[-1].strip() != '':
                        new_lines.append('') 
                    in_table = True
                # å†™å…¥å»ç¼©è¿›åçš„è¡Œ
                new_lines.append(stripped)
            else:
                if in_table:
                    # [é€€å‡ºè¡¨æ ¼]
                    # æ’å…¥ç©ºè¡Œ
                    if stripped != '':
                        new_lines.append('')
                    in_table = False
                # éè¡¨æ ¼è¡Œä¿æŒåŸæ · (ä¿ç•™åŸæœ‰çš„ç¼©è¿›)
                new_lines.append(line)
        return '\n'.join(new_lines)

    def _prepare_data_context(self, chapter: Dict, sec_title: str, custom_data: str, local_client, title: str) -> tuple:
        """è¾…åŠ©æ–¹æ³•ï¼šå‡†å¤‡æ•°æ®ä¸Šä¸‹æ–‡ (å«æ•°æ®è·¯ç”±ä¸è”ç½‘æœç´¢)"""
        facts_context = ""
        logs = []
        use_data_flag = chapter.get('use_data', False)
        has_user_data = False

        if "æ‘˜è¦" not in sec_title and use_data_flag:
            if custom_data and len(custom_data.strip()) > 5:
                cleaned_data = TextCleaner.convert_cn_numbers(custom_data)
                facts_context += f"""
\n================ ã€æœ¬ç ”ç©¶æ ¸å¿ƒè°ƒç ”æ•°æ®åº“ (Research Database)ã€‘ ================
{cleaned_data}
============================================================================

ã€âš ï¸ æ•°æ®ä½¿ç”¨æœ€é«˜æŒ‡ä»¤ (Data Usage & Integration Rules)ã€‘ï¼š
1. **æ™ºèƒ½è·¯ç”± (Smart Routing)**: 
   - ä¸Šæ–¹æ•°æ®åº“åŒ…å«å¤šä¸ª `<datasource>` (æ¥æºæ–‡ä»¶)ã€‚
   - è¯·æ ¹æ®å½“å‰ç« èŠ‚æ ‡é¢˜ **â€œ{sec_title}â€**ï¼Œæ™ºèƒ½ç­›é€‰å‡ºä¸æœ¬ç« ä¸»é¢˜**æœ€ç›¸å…³**çš„ä¸€ä¸ªæˆ–å‡ ä¸ªæ–‡ä»¶è¿›è¡Œåˆ†æã€‚
   - **ä¸¥ç¦ä¸²å‘³**: å¦‚æœæœ¬ç« è®²â€œè´¢åŠ¡â€ï¼Œè¯·å¿½ç•¥â€œäººå‘˜åå•â€ç±»çš„æ•°æ®ã€‚

2. **éšå½¢èå…¥ (Seamless Integration - CRITICAL)**:
   - **è§’è‰²è®¾å®š**: ä½ æ˜¯è®ºæ–‡çš„ä½œè€…ï¼Œè¿™äº›æ•°æ®æ˜¯ä½ **äº²è‡ªè°ƒç ”ã€æ”¶é›†å’Œæ•´ç†**çš„ä¸€æ‰‹èµ„æ–™ã€‚
   - **ç»å¯¹ç¦è¯­**: **ä¸¥ç¦**åœ¨æ­£æ–‡ä¸­å‡ºç°â€œç”¨æˆ·æä¾›â€ã€â€œä¸Šä¼ çš„æ–‡ä»¶â€ã€â€œæ ¹æ®ç»™å®šçš„æ•°æ®â€ã€â€œé™„ä»¶ä¸­â€ç­‰æ‰“ç ´å­¦æœ¯è¯­å¢ƒçš„è¯æ±‡ã€‚
   - **æ­£ç¡®å†™æ³•**: å°†æ•°æ®è½¬åŒ–ä¸ºè‡ªç„¶çš„å­¦æœ¯è®ºè¿°ã€‚
     - âŒ é”™è¯¯: â€œæ ¹æ®ç”¨æˆ·æä¾›çš„ã€Š2023è´¢æŠ¥ã€‹æ˜¾ç¤º...â€
     - âœ… æ­£ç¡®: â€œæ ¹æ®2023å¹´åº¦è´¢åŠ¡æŠ¥è¡¨æ•°æ®æ˜¾ç¤º...â€ / â€œæ•°æ®æ˜¾ç¤ºï¼Œ...â€ / â€œä»èµ„äº§è´Ÿå€ºæƒ…å†µæ¥çœ‹...â€
   - **å›¾è¡¨é…åˆ**: å¦‚æœæ–‡ä¸­åˆ—ä¸¾äº†å¤§é‡æ•°æ®ï¼Œè¯·ç”¨æ–‡å­—å¯¹æ•°æ®èƒŒåçš„**è¶‹åŠ¿ã€å æ¯”ã€å¼‚å¸¸å€¼**è¿›è¡Œåˆ†æï¼Œè€Œä¸ä»…ä»…æ˜¯æŠ¥è´¦ã€‚

3. **æ•°æ®å®è¯**: 
   - æœ¬ç« èŠ‚ **å¿…é¡»** å¼•ç”¨ä¸Šè¿°æ•°æ®åº“ä¸­çš„å…·ä½“æ•°å€¼ä½œä¸ºè®ºæ®ã€‚
   - æ²¡æœ‰æ•°æ®çš„è®ºè¿°æ˜¯ç©ºæ´çš„ï¼Œå¿…é¡»ç”¨æ•°æ®è¯´è¯ï¼ˆä¾‹å¦‚ï¼šâ€œå¢é•¿äº†15%â€ã€â€œå æ¯”è¾¾åˆ°40%â€ï¼‰ã€‚
"""
                has_user_data = True
            
            # è”ç½‘æœç´¢é€»è¾‘
            # facts = self._research_phase_with_client(local_client, f"{title} - {sec_title} æ•°æ®")
            # if facts:
            #    facts_context += f"\nã€è”ç½‘è¡¥å……æ•°æ®ã€‘:\n{facts}\n"
        
        return facts_context, has_user_data, logs

    def _prepare_ref_context(self, sec_title: str, ref_domestic: str, ref_foreign: str) -> tuple:
        """è¾…åŠ©æ–¹æ³•ï¼šå‡†å¤‡å‚è€ƒæ–‡çŒ®ä¸Šä¸‹æ–‡"""
        logs = []
        target_ref_list = []
        is_domestic_review = "å›½å†…" in sec_title and ("ç°çŠ¶" in sec_title or "ç»¼è¿°" in sec_title)
        is_foreign_review = "å›½å¤–" in sec_title and ("ç°çŠ¶" in sec_title or "ç»¼è¿°" in sec_title)
        
        raw_ref_text = ""
        if is_domestic_review:
            logs.append(f"   - ğŸ“š é”å®šï¼šå›½å†…å‚è€ƒæ–‡çŒ®")
            raw_ref_text = ref_domestic
        elif is_foreign_review:
            logs.append(f"   - ğŸ“š é”å®šï¼šå›½å¤–å‚è€ƒæ–‡çŒ®")
            raw_ref_text = ref_foreign
        else:
            raw_ref_text = f"{ref_domestic}\n{ref_foreign}"

        if raw_ref_text:
            target_ref_list = [line.strip() for line in raw_ref_text.split('\n') if line.strip()]
            
        return target_ref_list, logs

    def _generate_raw_content(self, client, title, sec_title, context_summary, target, 
                              facts_context, has_user_data, target_ref_list, 
                              full_outline_str, chart_type, extra_instructions) -> tuple:
        """è¾…åŠ©æ–¹æ³•ï¼šæ„å»º Prompt å¹¶è°ƒç”¨ LLM ç”ŸæˆåŸå§‹å†…å®¹"""
        logs = []
        chapter_num = self._extract_chapter_num(sec_title)
        # è‡ªåŠ¨æ£€æµ‹è¯­è¨€æ¨¡å¼ (ç”¨äºå†³å®š User Prompt çš„è¯­è¨€)
        import re
        is_chinese_mode = bool(re.search(r'[\u4e00-\u9fa5]', sec_title))
        # æ„å»º System Prompt (å†…éƒ¨ä¼šè‡ªåŠ¨åˆ†å‘ CN/EN)
        sys_prompt = get_academic_thesis_prompt(
            target, 
            target_ref_list, 
            sec_title, 
            chapter_num, 
            has_user_data, 
            full_outline=full_outline_str,
            chart_type=chart_type
        )
        # æ„å»º User Prompt 
        user_prompt = ""
        if is_chinese_mode:
            # ä¸­æ–‡æŒ‡ä»¤
            user_prompt = f"é¢˜ç›®ï¼š{title}\nç« èŠ‚ï¼š{sec_title}\nå‰æ–‡æ‘˜è¦ï¼š{context_summary}\nã€é‡è¦çº¦æŸã€‘ç›®æ ‡å­—æ•°ï¼š{target}å­—\n{facts_context}"
            if extra_instructions and len(extra_instructions.strip()) > 0:
                user_prompt += f"\n\nã€ç”¨æˆ·é¢å¤–å…·ä½“éœ€æ±‚ (æœ€é«˜ä¼˜å…ˆçº§)ã€‘\n{extra_instructions}\n"
        else:
            # è‹±æ–‡æŒ‡ä»¤ (Strict Translation)
            user_prompt = f"Thesis Title: {title}\nChapter: {sec_title}\nContext Summary: {context_summary}\n[Constraint] Target Word Count: {target}\n{facts_context}"
            if extra_instructions and len(extra_instructions.strip()) > 0:
                user_prompt += f"\n\n[User Extra Instructions (High Priority)]\n{extra_instructions}\n"
        # è°ƒç”¨ LLM
        content = self._call_llm_with_client(client, sys_prompt, user_prompt)
        # å­—æ•°æ‰©å†™æ£€æŸ¥ (åŒè¯­é€‚é…)
        content_no_code = re.sub(r'```[\s\S]*?```', '', content)
        current_len = len(re.sub(r'\s', '', content_no_code))
        # è‹±æ–‡å•è¯é€šå¸¸æ¯”æ±‰å­—å¤šï¼Œæ‰€ä»¥è‹±æ–‡æ¨¡å¼ä¸‹å­—æ•°é˜ˆå€¼å¯ä»¥é€‚å½“è°ƒæ•´ï¼Œæˆ–è€…æŒ‰å­—ç¬¦æ•°ä¼°ç®—
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œé€»è¾‘ä¿æŒä¸€è‡´
        if "abstract" not in sec_title.lower() and "æ‘˜è¦" not in sec_title and target > 300 and current_len < target * 0.5:
            try:
                logs.append(f"   - âš ï¸ Word count low ({current_len}/{target}), expanding...")
                expand_instruction = "\n\nè¯·å¤§å¹…æ‰©å†™ï¼Œå¢åŠ ç»†èŠ‚ï¼Œç¡®ä¿å­—æ•°è¾¾æ ‡ã€‚" if is_chinese_mode else "\n\nPlease expand significantly, adding details to meet the word count requirement."
                content = self._call_llm_with_client(client, sys_prompt, user_prompt + expand_instruction)
            except Exception as e:
                print(f"Expansion failed: {e}")
                
        return content, logs

    def _process_code_blocks(self, content: str) -> str:
        """è¾…åŠ©æ–¹æ³•ï¼šå¤„ç† Python ä»£ç å—ã€è‡ªåŠ¨é—­åˆä¸ç»˜å›¾æ‰§è¡Œ"""
        
        # 1. è‡ªåŠ¨é—­åˆä¿®å¤
        if content.count('```') % 2 != 0:
            content += "\n```"

        # 2. å®šä¹‰å®½å®¹çš„æ­£åˆ™
        code_block_pattern = re.compile(r'(```\s*(?:python|py)?\s*[\s\S]*?```)', re.IGNORECASE)

        def replacer(match):
            full_block = match.group(1)
            # æå–çº¯ä»£ç 
            lines = full_block.strip().split('\n')
            code_lines = [line for line in lines if '```' not in line]
            
            if not code_lines: return match.group(0)
            
            code = '\n'.join(code_lines).strip()
            if not code: return match.group(0)

            try:
                # æ‰§è¡Œç»˜å›¾
                img_buf = MarkdownToDocx.exec_python_plot(code)
                if img_buf:
                    b64_data = base64.b64encode(img_buf.getvalue()).decode('utf-8')
                    return f"\n![ç»Ÿè®¡å›¾](data:image/png;base64,{b64_data})\n"
                else:
                    return match.group(0)
            except Exception as e:
                print(f"Plot Execution Error: {e}")
                return match.group(0)

        # æ‰§è¡Œæ›¿æ¢
        return code_block_pattern.sub(replacer, content)

    def _process_single_chapter(self, task_bundle):
        """çº¿ç¨‹å·¥ä½œå‡½æ•° (é‡æ„ç‰ˆ)"""
        i = -1
        sec_title = "æœªçŸ¥ç« èŠ‚"
        logs = []
        
        try:
            # 1. å‚æ•°è§£åŒ…ä¸æ ¡éªŒ
            if len(task_bundle) < 13: 
                return { "index": -1, "type": "error", "msg": f"å‚æ•°ä¸è¶³: {len(task_bundle)}", "logs": [] }

            (api_key, base_url, model, task_id, title, chapter, 
             ref_domestic, ref_foreign, 
             custom_data, context_summary, index_val, 
             full_outline_str, extra_instructions) = task_bundle
            
            i = index_val
            sec_title = chapter.get('title', 'æ— æ ‡é¢˜')
            target = int(chapter.get('words', 500))
            is_parent = chapter.get('is_parent', False)
            chart_type = chapter.get('chart_type', 'none')

            # 2. æ ‡é¢˜ä¸å±‚çº§å¤„ç†
            header_prefix = self._determine_header_prefix(chapter, sec_title)
            if is_parent or target <= 0:
                return {
                    "index": i, "type": "header_only", 
                    "content": f"{header_prefix} {sec_title}\n\n",
                    "logs": [f"ç”Ÿæˆæ ‡é¢˜: {sec_title}"]
                }

            # 3. åˆå§‹åŒ– Client
            local_client = OpenAI(api_key=api_key, base_url=base_url, timeout=120.0)
            logs.append(f"ğŸš€ [å¹¶å‘å¯åŠ¨] æ­£åœ¨æ’°å†™: {sec_title}")

            # 4. å‡†å¤‡ä¸Šä¸‹æ–‡ (æ•°æ® + æ–‡çŒ®)
            facts_context, has_user_data, data_logs = self._prepare_data_context(
                chapter, sec_title, custom_data, local_client, title
            )
            logs.extend(data_logs)

            target_ref_list, ref_logs = self._prepare_ref_context(
                sec_title, ref_domestic, ref_foreign
            )
            logs.extend(ref_logs)

            # 5. ç”Ÿæˆæ ¸å¿ƒå†…å®¹ (å«æ‰©å†™é‡è¯•)
            content, gen_logs = self._generate_raw_content(
                local_client, title, sec_title, context_summary, target,
                facts_context, has_user_data, target_ref_list,
                full_outline_str, chart_type, extra_instructions
            )
            logs.extend(gen_logs)

            # 6. åå¤„ç† (ä»£ç æ‰§è¡Œã€æ ¼å¼æ¸…æ´—)
            content = self._process_code_blocks(content)
            content = self._clean_and_format(content, sec_title, None)
            final_content = self._fix_markdown_table_format(content)
            
            # 7. ç»„è£…ç»“æœ
            section_md = f"{header_prefix} {sec_title}\n\n{final_content}\n\n"

            return {
                "index": i, "type": "content", 
                "content": section_md, "raw_text": final_content, "logs": logs
            }

        except Exception as e:
            err_msg = f"âŒ {sec_title} å¼‚å¸¸: {str(e)}"
            print(f"[Thread {i}] ERROR: {err_msg}")
            # æ‰“å°è¯¦ç»†å †æ ˆä»¥ä¾¿è°ƒè¯•
            import traceback
            traceback.print_exc()
            return { "index": i, "type": "error", "msg": str(e), "logs": [err_msg] }
        
    def write_section_content(self, 
                              section_title: str, 
                              word_count: int, 
                              references: List[str], 
                              full_outline_str: str,
                              chapter_num: str,
                              has_data: bool = False,
                              opening_report: Optional[Dict] = None) -> Generator[str, None, None]:
        """
        æµå¼ç”Ÿæˆç« èŠ‚å†…å®¹
        :param opening_report: è§£æåçš„å¼€é¢˜æŠ¥å‘Šå­—å…¸ (title, review, outline_content)
        """
        
        # 1. ç­–ç•¥ A: ç›´æ¥å†…å®¹å¤ç”¨ (Direct Hit)
        # å¦‚æœå½“å‰ç« èŠ‚æ˜¯â€œæ–‡çŒ®ç»¼è¿°â€ä¸”å¼€é¢˜æŠ¥å‘Šé‡Œæœ‰å¤§æ®µç»¼è¿°ï¼Œå¯ä»¥è€ƒè™‘ç›´æ¥è¿”å›
        # ä½†ä¸ºäº†ä¿æŒæ–‡é£ç»Ÿä¸€ï¼Œè¿™é‡Œé€‰æ‹©å°†å¼€é¢˜æŠ¥å‘Šä½œä¸º Context ä¼ å…¥ Prompt (Strategy I)ï¼Œ
        # è®© LLM è¿›è¡Œæ¶¦è‰²å’Œæ‰©å†™ï¼Œè€Œä¸æ˜¯ç”Ÿç¡¬çš„ Copy-Pasteã€‚
        
        # 2. æ„å»ºç³»ç»Ÿæç¤ºè¯ (åŒ…å«å¼€é¢˜æŠ¥å‘Šçº¦æŸ)
        system_prompt = get_academic_thesis_prompt(
            target_words=word_count,
            ref_content_list=references,
            current_chapter_title=section_title,
            chapter_num=chapter_num,
            has_user_data=has_data,
            full_outline=full_outline_str,
            opening_report_data=opening_report # <--- ä¼ å…¥å¼€é¢˜æŠ¥å‘Šæ•°æ®
        )

        user_prompt = f"è¯·æ’°å†™ç« èŠ‚ï¼šã€{section_title}ã€‘\nè¦æ±‚å­—æ•°ï¼šçº¦ {word_count} å­—ã€‚"
        
        # 3. æµå¼è°ƒç”¨
        for chunk in self._call_llm_stream_with_client(self.main_client, system_prompt, user_prompt):
            yield chunk

    def _format_outline(self, chapters: List[Dict]) -> str:
        outline_lines = []
        for ch in chapters:
            title = ch.get('title', 'æœªå‘½å')
            outline_lines.append(f"- {title}")
        return "\n".join(outline_lines)

    def generate_stream(
            self, 
            task_id: str, 
            title: str, 
            chapters: List[Dict], 
            ref_domestic: str, 
            ref_foreign: str, 
            custom_data: str, 
            check_status_func, 
            initial_context: str = "", 
            extra_instructions: str = ""
            ) -> Generator[str, None, None]:
        
        # è¿™é‡Œçš„ ref_manager ä¸»è¦ç”¨äºæœ€åç”Ÿæˆæ–‡æœ«çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼Œæ‰€ä»¥åˆå¹¶ä¸¤è€…
        combined_refs = f"{ref_domestic}\n{ref_foreign}"
        ref_manager = ReferenceManager(combined_refs)
        yield f"data: {json.dumps({'type': 'log', 'msg': 'ğŸš€ å¯åŠ¨é«˜å¹¶å‘ç”Ÿæˆå¼•æ“ (Max Threads=8)...'})}\n\n"
        full_content = f"# {title}\n\n"
        global_context = initial_context if initial_context else f"è®ºæ–‡é¢˜ç›®ï¼šã€Š{title}ã€‹"
        
        # é¢„å…ˆç”Ÿæˆå…¨æ–‡å¤§çº²æ–‡æœ¬å­—ç¬¦ä¸²
        full_outline_str = self._format_outline(chapters)

        MAX_WORKERS = 8
        all_futures = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # 1. æäº¤ä»»åŠ¡
            for i, chapter in enumerate(chapters):
                if self._check_process_status(check_status_func): break
                
                task_bundle = (
                    self.api_key, self.base_url, self.model,
                    task_id, title, chapter, 
                    ref_domestic, ref_foreign,  # <--- æ–°å¢çš„ä¸¤ä¸ªå‚æ•°
                    custom_data, global_context[:800], i,
                    full_outline_str,
                    extra_instructions
                )
                future = executor.submit(self._process_single_chapter, task_bundle)
                all_futures.append(future)
            
            # 2. è·å–ç»“æœ
            for future in all_futures:
                if self._check_process_status(check_status_func):
                    executor.shutdown(wait=False)
                    break
                while True:
                    try:
                        result = future.result(timeout=1)
                        for log in result.get('logs', []):
                            yield f"data: {json.dumps({'type': 'log', 'msg': log})}\n\n"
                        if result['type'] == 'error':
                            yield f"data: {json.dumps({'type': 'log', 'msg': result['msg']})}\n\n"
                            break
                        if result['type'] in ['content', 'header_only']:
                            content_md = result['content']
                            full_content += content_md
                            yield f"data: {json.dumps({'type': 'content', 'md': content_md})}\n\n"
                            global_context += result.get('raw_text', '')[-200:]
                        break
                    except concurrent.futures.TimeoutError:
                        yield f": keep-alive\n\n"
                        if self._check_process_status(check_status_func): return
                    except Exception as e:
                        yield f"data: {json.dumps({'type': 'log', 'msg': f'âŒ ä¸»çº¿ç¨‹å¼‚å¸¸: {str(e)}'})}\n\n"
                        break

        if check_status_func() != "stopped":
            # ç”Ÿæˆæ–‡æœ«å‚è€ƒæ–‡çŒ®åˆ—è¡¨
            bib = ref_manager.generate_bibliography()
            full_content += bib
            yield f"data: {json.dumps({'type': 'content', 'md': bib})}\n\n"
            yield f"data: {json.dumps({'type': 'done'})}\n\n"

    def _process_uploaded_files(self, files):
        """
        å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨ï¼š
        1. æ–‡æ¡£ç±» (PDF, DOCX, TXT, CSV) -> æå–ä¸ºæ–‡æœ¬å­—ç¬¦ä¸²
        2. å›¾ç‰‡ç±» -> ä¿ç•™åŸå§‹å¯¹è±¡ (ä»¥ä¾¿ä¼ ç»™ Vision æ¨¡å‹)
        """
        if not files:
            return "", []

        extracted_text = []
        image_files = []

        for f in files:
            filename = f.get('name', '').lower()
            content_stream = f.get('content') # è¿™æ˜¯ä¸€ä¸ª BytesIO å¯¹è±¡
            
            if not content_stream:
                continue
                
            # é‡ç½®æŒ‡é’ˆ
            content_stream.seek(0)

            # --- A. å›¾ç‰‡å¤„ç† ---
            if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp')):
                # å°†å›¾ç‰‡æ•°æ®è½¬ä¸º base64 æˆ–ç›´æ¥ä¼ é€’æµï¼Œå–å†³äºä½ çš„ _call_llm å®ç°
                # è¿™é‡Œæˆ‘ä»¬ä¼ é€’åŸå§‹ dictï¼Œè®©åº•å±‚å†³å®šæ€ä¹ˆå¤„ç†
                image_files.append(f)
            
            # --- B. æ–‡æ¡£å¤„ç† ---
            elif filename.endswith('.docx') and Document:
                try:
                    doc = Document(content_stream)
                    text = "\n".join([para.text for para in doc.paragraphs])
                    extracted_text.append(f"ã€å‚è€ƒæ–‡æ¡£ï¼š{filename}ã€‘\n{text}")
                except Exception as e:
                    print(f"Error parsing docx {filename}: {e}")
            
            elif filename.endswith('.pdf') and PdfReader:
                try:
                    reader = PdfReader(content_stream)
                    text = ""
                    for page in reader.pages:
                        text += page.extract_text() + "\n"
                    extracted_text.append(f"ã€å‚è€ƒæ–‡æ¡£ï¼š{filename}ã€‘\n{text}")
                except Exception as e:
                    print(f"Error parsing pdf {filename}: {e}")
            
            elif filename.endswith(('.txt', '.csv', '.md')):
                try:
                    text = content_stream.read().decode('utf-8', errors='ignore')
                    extracted_text.append(f"ã€å‚è€ƒæ–‡æ¡£ï¼š{filename}ã€‘\n{text}")
                except Exception as e:
                    print(f"Error reading text {filename}: {e}")

        return "\n\n".join(extracted_text), image_files
    
    def rewrite_chapter(self, title: str, section_title: str, user_instruction: str, context: str, custom_data: str, original_content: str = "", files: list = None) -> str:
        # ã€ä¿®æ”¹ç‚¹ 1ã€‘å¢åŠ äº† files å‚æ•°ï¼Œé»˜è®¤ä¸º None
        
        chapter_num = self._extract_chapter_num(section_title)
        
        # ã€ä¿®æ”¹ç‚¹ 2ã€‘å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
        # æå–æ–‡æ¡£æ–‡æœ¬ å’Œ å›¾ç‰‡åˆ—è¡¨
        file_text_context, image_files = self._process_uploaded_files(files)
        
        # ã€ä¿®æ”¹ç‚¹ 3ã€‘å°†æå–çš„æ–‡æ¡£å†…å®¹ï¼Œåˆå¹¶åˆ° custom_data æˆ– instruction ä¸­
        # è¿™æ · LLM å°±èƒ½â€œè¯»â€åˆ° Word/PDF çš„å†…å®¹äº†
        if file_text_context:
            custom_data = f"{custom_data}\n\n{file_text_context}"
            # æˆ–è€…è¿½åŠ åˆ° instructionï¼ŒåŠ å¼ºæç¤º
            user_instruction += f"\n\n(è¯·å‚è€ƒæˆ‘ä¸Šä¼ çš„é™„ä»¶æ–‡æ¡£å†…å®¹è¿›è¡Œæ’°å†™)"

        # 1. æ„å»º Prompt
        sys_prompt = get_rewrite_prompt(title, section_title, user_instruction, context[-800:], custom_data, original_content, chapter_num)
        
        user_prompt = f"è®ºæ–‡é¢˜ç›®ï¼š{title}\nè¯·ä¿®æ”¹ç« èŠ‚ï¼š{section_title}\nç”¨æˆ·çš„å…·ä½“ä¿®æ”¹æ„è§ï¼š{user_instruction}\nã€æœ€é«˜æŒ‡ä»¤ã€‘ç›´æ¥è¾“å‡ºæ­£æ–‡ã€‚å¦‚æœéœ€è¦ç»˜å›¾ï¼Œè¯·è¾“å‡ºå®Œæ•´çš„ Markdown ä»£ç å— (```python ... ```)ï¼Œä¸è¦è§£é‡Šä»£ç ã€‚"
        
        # ã€ä¿®æ”¹ç‚¹ 4ã€‘è°ƒç”¨ LLM æ—¶ä¼ å…¥ images
        # æ³¨æ„ï¼šä½ éœ€è¦ç¡®ä¿ self._call_llm æ–¹æ³•èƒ½å¤Ÿæ¥æ”¶ images å‚æ•°å¹¶ä¼ é€’ç»™ GPT-4o/Claude
        if image_files:
            content = self._call_llm(sys_prompt, user_prompt, images=image_files)
        else:
            content = self._call_llm(sys_prompt, user_prompt)

        # 3. [Step 1] æ¸…æ´—åºŸè¯æ ‡é¢˜
        garbage_patterns = [
            r'^\s*(?:#+|\*\*|)?\s*(?:è®¾ç½®|å®šä¹‰|åˆ›å»º|ç»˜åˆ¶|æ·»åŠ |å¯¼å…¥|å‡†å¤‡)(?:ç»˜å›¾)?(?:é£æ ¼|æ•°æ®|å˜é‡|ç”»å¸ƒ|æ¡å½¢å›¾|æŠ˜çº¿å›¾|é¥¼å›¾|ç»Ÿè®¡å›¾|å›¾è¡¨|æ•°å€¼|æ ‡ç­¾|å¼•ç”¨|ç›¸å…³åº“|ä»£ç ).*?$',
            r'^\s*(?:#+|\*\*|)?\s*Python\s*ä»£ç (?:å¦‚ä¸‹|ç¤ºä¾‹)?[:ï¼š]?\s*$',
            r'^\s*(?:#+|\*\*|)?\s*ä»£ç å¦‚ä¸‹[:ï¼š]?\s*$'
        ]
        for pat in garbage_patterns:
            content = re.sub(pat, '', content, flags=re.MULTILINE | re.IGNORECASE)

        # =========================================================
        # [Step 2] æ ¸å¿ƒä¿®å¤ï¼šè‡ªåŠ¨è¡¥å…¨ä¸å®½å®¹åŒ¹é…
        # =========================================================
        
        # A. è‡ªåŠ¨é—­åˆä¿®å¤ï¼šå¦‚æœä»£ç å—æ ‡è®°æ˜¯å¥‡æ•°ä¸ªï¼Œè¯´æ˜ LLM æ²¡å†™å®Œï¼Œå¼ºåˆ¶è¡¥å…¨
        if content.count('```') % 2 != 0:
            content += "\n```"

        # B. å®½å®¹æ­£åˆ™ï¼šå…è®¸ ```python, ``` python, ç”šè‡³ä¸å†™ python çš„ ``` 
        code_block_pattern = re.compile(r'(```\s*(?:python|py)?\s*[\s\S]*?```)', re.IGNORECASE)
        
        def image_replacer(match):
            full_block = match.group(1).strip()
            
            # æå–å†…éƒ¨ä»£ç 
            lines = full_block.split('\n')
            
            # è¿‡æ»¤æ‰ç¬¬ä¸€è¡Œ (```xxx) å’Œæœ€åä¸€è¡Œ (```)
            code_lines = [line for line in lines if '```' not in line]
            
            if not code_lines: return "" # ç©ºå—
            
            code = '\n'.join(code_lines).strip()
            if not code: return ""

            try:
                # æ‰§è¡Œç»˜å›¾
                # ç¡®ä¿å¼•å…¥äº† MarkdownToDocx æˆ–ç›¸åº”çš„ç»˜å›¾å·¥å…·
                img_buf = MarkdownToDocx.exec_python_plot(code)
                if img_buf:
                    b64_data = base64.b64encode(img_buf.getvalue()).decode('utf-8')
                    # è¿”å›å›¾ç‰‡ HTML
                    return f'\n\n<div align="center" class="plot-container"><img src="data:image/png;base64,{b64_data}" style="max-width:85%; border:1px solid #eee; padding:5px; border-radius:4px;"></div>\n\n'
                else:
                    return "" 
            except Exception as e:
                print(f"Plot Logic Error: {e}")
                return ""

        # æ‰§è¡Œæ›¿æ¢
        new_content = code_block_pattern.sub(image_replacer, content)
        
        # [Step 3] æœ€åçš„æ‰«å°¾
        new_content = re.sub(r'\n{3,}', '\n\n', new_content)

        return new_content.strip()

    def plan_word_count(self, total_words: int, outline_list: List[str]) -> Dict[str, Dict]:
        outline_str = "\n".join(outline_list)
        prompt = get_word_distribution_prompt(total_words, outline_str)
        
        try:
            response = self.main_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼è¾“å‡º JSON çš„å­¦æœ¯è§„åˆ’å¸ˆã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                stream=False,
                response_format={"type": "json_object"}
            )
            content = response.choices[0].message.content.strip()
            if content.startswith("```"): content = re.sub(r'```json|```', '', content).strip()
            
            try:
                raw_map = json.loads(content)
            except json.JSONDecodeError:
                return {}

            standardized_map = {}
            for k, v in raw_map.items():
                if "total" in k.lower(): continue
                if isinstance(v, dict):
                    w = int(v.get('words', 0))
                    d = v.get('needs_data', False)
                    if isinstance(d, str): d = d.lower() == 'true'
                    standardized_map[k] = {"words": w, "needs_data": d}
                elif isinstance(v, (int, float)):
                    standardized_map[k] = {"words": int(v), "needs_data": False}
            
            current_total = sum(item['words'] for item in standardized_map.values())
            if current_total == 0: return standardized_map

            ratio = total_words / current_total
            final_map = {}
            for k, v in standardized_map.items():
                final_map[k] = {"words": int(v['words'] * ratio), "needs_data": v['needs_data']}
            
            # è¯¯å·®ä¿®æ­£
            new_total = sum(item['words'] for item in final_map.values())
            diff = total_words - new_total
            if diff != 0 and final_map:
                max_key = max(final_map, key=lambda k: final_map[k]['words'])
                final_map[max_key]['words'] += diff
                
            return final_map
        except Exception as e:
            print(f"Plan error: {e}")
            return {}